from flask import (
    Flask,
    jsonify,
    redirect,
    render_template,
    request,
    send_from_directory,
    url_for,
)
from flask import session
import pandas as pd
from datetime import datetime, time, timedelta, date
import pytz
import os
import copy
import shutil
import re
from pathlib import Path
from threading import Lock

import pandas as pd
import pytz
import yaml
import json
from functools import wraps
from typing import Dict, Any, Optional

import logging
from logging.handlers import RotatingFileHandler
from typing import Any, Callable, Dict, List, Optional, Tuple
from apscheduler.schedulers.background import BackgroundScheduler
from apscheduler.triggers.cron import CronTrigger

os.makedirs('logs', exist_ok=True)

selection_logger = logging.getLogger('selection')
selection_logger.setLevel(logging.INFO)

handler = RotatingFileHandler('logs/selection.log', maxBytes=10_000_000, backupCount=3)
handler.setFormatter(logging.Formatter('%(asctime)s [%(levelname)s] %(message)s'))
selection_logger.addHandler(handler)


app = Flask(__name__)
app.config['UPLOAD_FOLDER'] = 'uploads'
app.static_folder = 'static'
# Set a secret key for sessions (make sure to set a secure key in production)
app.secret_key = 'your-maxsecret-key'


if not os.path.exists(app.config['UPLOAD_FOLDER']):
    os.makedirs(app.config['UPLOAD_FOLDER'])

# Master CSV path for auto-preload
MASTER_CSV_PATH = os.path.join(app.config['UPLOAD_FOLDER'], 'master_medweb.csv')

lock = Lock()

# Scheduler for auto-preload (initialized later after modality_data is loaded)
scheduler = None

# JSON worker skill roster (loaded from worker_skill_overrides.json)
# Takes priority over YAML config.worker_skill_roster
worker_skill_json_roster = {}

# -----------------------------------------------------------
# Global constants & modality-/skill-specific factors
# -----------------------------------------------------------

DEFAULT_FALLBACK_CHAIN = {
    'Normal': [],
    'Notfall': ['Normal'],
    'Herz': ['Notfall', 'Normal'],
    'Privat': ['Notfall', 'Normal'],
    'Msk': ['Notfall', 'Normal'],
    'Chest': ['Notfall', 'Normal']
}

DEFAULT_SKILLS = {
    "Normal": {
        "label": "Normal",
        "button_color": "#004892",
        "text_color": "#ffffff",
        "weight": 1.0,
        "optional": False,
        "special": False,
        "always_visible": True,
        "fallback": [],
    },
    "Notfall": {
        "label": "Notfall",
        "button_color": "#dc3545",
        "text_color": "#ffffff",
        "weight": 1.1,
        "optional": False,
        "special": False,
        "always_visible": True,
        "fallback": ["Normal"],
    },
    "Privat": {
        "label": "Privat",
        "button_color": "#ffc107",
        "text_color": "#333333",
        "weight": 1.2,
        "optional": True,
        "special": False,
        "always_visible": True,
        "fallback": ["Notfall", "Normal"],
    },
    "Herz": {
        "label": "Herz",
        "button_color": "#28a745",
        "text_color": "#ffffff",
        "weight": 1.2,
        "optional": True,
        "special": True,
        "always_visible": False,
        "fallback": ["Notfall", "Normal"],
    },
    "Msk": {
        "label": "Msk",
        "button_color": "#9c27b0",
        "text_color": "#ffffff",
        "weight": 0.8,
        "optional": True,
        "special": True,
        "always_visible": False,
        "fallback": ["Notfall", "Normal"],
    },
    "Chest": {
        "label": "Chest",
        "button_color": "#ff9800",
        "text_color": "#ffffff",
        "weight": 0.8,
        "optional": True,
        "special": True,
        "always_visible": False,
        "fallback": ["Notfall", "Normal"],
    },
}

DEFAULT_MODALITIES = {
    'ct': {
        'label': 'CT',
        'nav_color': '#1a5276',
        'hover_color': '#153f5b',
        'background_color': '#e6f2fa',
        'factor': 1.0,
    },
    'mr': {
        'label': 'MR',
        'nav_color': '#777777',
        'hover_color': '#555555',
        'background_color': '#f9f9f9',
        'factor': 1.2,
    },
    'xray': {
        'label': 'XRAY',
        'nav_color': '#239b56',
        'hover_color': '#1d7a48',
        'background_color': '#e0f2e9',
        'factor': 0.33,
    },
}

DEFAULT_CONFIG = {
    'admin_password': 'change_pw_for_live',
    'modalities': DEFAULT_MODALITIES,
    'skills': DEFAULT_SKILLS,
    'modality_fallbacks': {},
    'balancer': {}
}

DEFAULT_BALANCER = {
    'enabled': True,
    'min_assignments_per_skill': 5,
    'imbalance_threshold_pct': 30,
    'allow_fallback_on_imbalance': True,
    'fallback_strategy': 'skill_priority',
    'fallback_chain': DEFAULT_FALLBACK_CHAIN
}


def _normalize_skill_fallback_entries(entries: Any) -> List[Any]:
    """Normalize fallback tiers to support strings or nested groups."""

    normalized: List[Any] = []
    if not isinstance(entries, list):
        return normalized

    for entry in entries:
        if isinstance(entry, list):
            group: List[str] = []
            seen: set = set()
            for candidate in entry:
                if isinstance(candidate, str) and candidate not in seen:
                    group.append(candidate)
                    seen.add(candidate)
            if group:
                normalized.append(group)
        elif isinstance(entry, str):
            normalized.append(entry)

    return normalized


def _normalize_modality_fallback_entries(
    entries: Any,
    source_modality: str,
    valid_modalities: List[str],
) -> List[Any]:
    normalized: List[Any] = []
    if not isinstance(entries, list):
        return normalized

    valid_set = {m.lower(): m for m in valid_modalities}
    source_key = source_modality.lower()

    def _resolve(value: str) -> Optional[str]:
        key = value.lower()
        if key == source_key:
            return None
        return valid_set.get(key)

    for entry in entries:
        if isinstance(entry, list):
            group: List[str] = []
            seen: set = set()
            for candidate in entry:
                if not isinstance(candidate, str):
                    continue
                resolved = _resolve(candidate)
                if resolved and resolved not in seen:
                    group.append(resolved)
                    seen.add(resolved)
            if group:
                normalized.append(group)
        elif isinstance(entry, str):
            resolved = _resolve(entry)
            if resolved:
                normalized.append(resolved)

    return normalized


def _coerce_float(value: Any, default: float = 1.0) -> float:
    try:
        return float(value)
    except (TypeError, ValueError):
        return default


def _coerce_int(value: Any, default: int = 0) -> int:
    try:
        return int(value)
    except (TypeError, ValueError):
        return default


def _load_raw_config() -> Dict[str, Any]:
    try:
        with open('config.yaml', 'r', encoding='utf-8') as config_file:
            return yaml.safe_load(config_file) or {}
    except FileNotFoundError:
        return {}
    except Exception as exc:
        selection_logger.warning("Failed to load config.yaml: %s", exc)
        return {}


def load_worker_skill_json(use_staged: bool = False) -> Dict[str, Any]:
    """
    Load worker skill overrides from JSON file.

    Args:
        use_staged: If True, load from staged file. If False, load from active file.
    """
    filename = 'worker_skill_overrides_staged.json' if use_staged else 'worker_skill_overrides.json'
    try:
        with open(filename, 'r', encoding='utf-8') as json_file:
            data = json.load(json_file)
            selection_logger.info(f"Loaded worker skill overrides from {filename}: {len(data)} workers")
            return data
    except FileNotFoundError:
        selection_logger.info(f"No {filename} found, using YAML config only")
        return {}
    except Exception as exc:
        selection_logger.warning(f"Failed to load {filename}: {exc}")
        return {}


def save_worker_skill_json(roster_data: Dict[str, Any], use_staged: bool = False) -> bool:
    """
    Save worker skill overrides to JSON file.

    Args:
        roster_data: Roster data to save
        use_staged: If True, save to staged file. If False, save to active file.
    """
    filename = 'worker_skill_overrides_staged.json' if use_staged else 'worker_skill_overrides.json'
    try:
        with open(filename, 'w', encoding='utf-8') as json_file:
            json.dump(roster_data, json_file, indent=2, ensure_ascii=False)
        selection_logger.info(f"Saved worker skill overrides to {filename}: {len(roster_data)} workers")
        return True
    except Exception as exc:
        selection_logger.error(f"Failed to save {filename}: {exc}")
        return False


def get_merged_worker_roster(config: Dict[str, Any]) -> Dict[str, Any]:
    """
    Get merged worker skill roster from YAML config and JSON overrides.
    JSON takes priority over YAML.
    """
    # Start with YAML config
    yaml_roster = config.get('worker_skill_roster', {})

    # Merge with JSON roster (JSON has priority)
    merged = copy.deepcopy(yaml_roster)

    for worker_id, worker_data in worker_skill_json_roster.items():
        if worker_id in merged:
            # Merge worker data (JSON overrides YAML)
            for key, value in worker_data.items():
                if isinstance(value, dict) and key in merged[worker_id]:
                    # Deep merge for default/modality-specific sections
                    merged[worker_id][key].update(value)
                else:
                    merged[worker_id][key] = value
        else:
            # New worker only in JSON
            merged[worker_id] = copy.deepcopy(worker_data)

    return merged


def _build_app_config() -> Dict[str, Any]:
    raw_config = _load_raw_config()
    config: Dict[str, Any] = {
        'admin_password': raw_config.get('admin_password', DEFAULT_CONFIG['admin_password'])
    }

    merged_modalities: Dict[str, Dict[str, Any]] = {
        key: dict(values)
        for key, values in DEFAULT_MODALITIES.items()
    }
    user_modalities = raw_config.get('modalities') or {}
    if isinstance(user_modalities, dict):
        for key, override in user_modalities.items():
            base = merged_modalities.get(key, {}).copy()
            if isinstance(override, dict):
                base.update(override)
            merged_modalities[key] = base

    for key, values in merged_modalities.items():
        values.setdefault('label', key.upper())
        values.setdefault('nav_color', '#004892')
        values.setdefault('hover_color', values['nav_color'])
        values.setdefault('background_color', '#f0f0f0')
        values['factor'] = _coerce_float(values.get('factor', 1.0))

    config['modalities'] = merged_modalities

    merged_skills: Dict[str, Dict[str, Any]] = {
        key: dict(values)
        for key, values in DEFAULT_SKILLS.items()
    }
    user_skills = raw_config.get('skills') or {}
    if isinstance(user_skills, dict):
        for key, override in user_skills.items():
            base = merged_skills.get(key, {}).copy()
            if isinstance(override, dict):
                base.update(override)
            merged_skills[key] = base

    for key, values in merged_skills.items():
        values.setdefault('label', key)
        values.setdefault('button_color', '#004892')
        values.setdefault('text_color', '#ffffff')
        values['weight'] = _coerce_float(values.get('weight', 1.0))
        values.setdefault('optional', False)
        values.setdefault('special', False)
        values.setdefault('always_visible', False)
        fallback_value = values.get('fallback', DEFAULT_FALLBACK_CHAIN.get(key, []))
        values['fallback'] = _normalize_skill_fallback_entries(fallback_value)
        values['display_order'] = _coerce_int(values.get('display_order', 0))
        slug = values.get('slug') or key.lower().replace(' ', '_')
        values['slug'] = slug
        values.setdefault('form_key', slug)

    config['skills'] = merged_skills

    balancer_settings: Dict[str, Any] = copy.deepcopy(DEFAULT_BALANCER)
    user_balancer = raw_config.get('balancer')
    if isinstance(user_balancer, dict):
        for key, value in user_balancer.items():
            if key == 'fallback_chain' and isinstance(value, dict):
                merged_fallback = {}
                all_skills = set(DEFAULT_FALLBACK_CHAIN) | set(value)
                for skill in all_skills:
                    merged_fallback[skill] = _normalize_skill_fallback_entries(
                        list(DEFAULT_FALLBACK_CHAIN.get(skill, []))
                    )

                for skill, overrides in value.items():
                    if isinstance(overrides, list):
                        merged_fallback[skill] = _normalize_skill_fallback_entries(overrides)
                    else:
                        merged_fallback[skill] = merged_fallback.get(skill, [])

                balancer_settings['fallback_chain'] = merged_fallback
            else:
                balancer_settings[key] = value
    config['balancer'] = balancer_settings

    modality_fallbacks = raw_config.get('modality_fallbacks')
    normalized_fallbacks: Dict[str, List[Any]] = {}
    if isinstance(modality_fallbacks, dict):
        for mod, fallback_list in modality_fallbacks.items():
            normalized_fallbacks[mod.lower()] = _normalize_modality_fallback_entries(
                fallback_list,
                mod,
                list(merged_modalities.keys()),
            )
    config['modality_fallbacks'] = normalized_fallbacks
    return config


APP_CONFIG = _build_app_config()
MODALITY_SETTINGS = APP_CONFIG['modalities']
SKILL_SETTINGS = APP_CONFIG['skills']
allowed_modalities = list(MODALITY_SETTINGS.keys()) or list(DEFAULT_MODALITIES.keys())
default_modality = allowed_modalities[0] if allowed_modalities else 'ct'
modality_labels = {
    mod: settings.get('label', mod.upper())
    for mod, settings in MODALITY_SETTINGS.items()
}
modality_factors = {
    mod: settings.get('factor', 1.0)
    for mod, settings in MODALITY_SETTINGS.items()
}


def _build_skill_metadata(skills_config: Dict[str, Dict[str, Any]]) -> Tuple[List[str], Dict[str, str], Dict[str, str], List[Dict[str, Any]], Dict[str, float]]:
    ordered_skills = sorted(
        skills_config.items(),
        key=lambda item: (_coerce_int(item[1].get('display_order', 0)), item[0])
    )

    columns: List[str] = []
    slug_map: Dict[str, str] = {}
    form_keys: Dict[str, str] = {}
    templates: List[Dict[str, Any]] = []
    weights: Dict[str, float] = {}

    for name, data in ordered_skills:
        slug = data.get('slug') or name.lower().replace(' ', '_')
        form_key = data.get('form_key') or slug

        columns.append(name)
        slug_map[name] = slug
        form_keys[name] = form_key
        weights[name] = _coerce_float(data.get('weight', 1.0))

        templates.append({
            'name': name,
            'label': data.get('label', name),
            'slug': slug,
            'form_key': form_key,
            'button_color': data.get('button_color', '#004892'),
            'text_color': data.get('text_color', '#ffffff'),
            'optional': bool(data.get('optional', False)),
            'special': bool(data.get('special', False)),
            'always_visible': bool(data.get('always_visible', False)),
        })

    return columns, slug_map, form_keys, templates, weights


SKILL_COLUMNS, SKILL_SLUG_MAP, SKILL_FORM_KEYS, SKILL_TEMPLATES, skill_weights = _build_skill_metadata(SKILL_SETTINGS)
BALANCER_SETTINGS = APP_CONFIG.get('balancer', DEFAULT_BALANCER)
raw_balancer_chain = BALANCER_SETTINGS.get('fallback_chain', DEFAULT_FALLBACK_CHAIN)
BALANCER_FALLBACK_CHAIN = {}
if isinstance(raw_balancer_chain, dict):
    for skill, entries in raw_balancer_chain.items():
        BALANCER_FALLBACK_CHAIN[skill] = _normalize_skill_fallback_entries(entries)
else:
    BALANCER_FALLBACK_CHAIN = {k: _normalize_skill_fallback_entries(v) for k, v in DEFAULT_FALLBACK_CHAIN.items()}

BALANCER_SETTINGS['fallback_chain'] = BALANCER_FALLBACK_CHAIN
RAW_MODALITY_FALLBACKS = APP_CONFIG.get('modality_fallbacks', {})
MODALITY_FALLBACK_CHAIN = {}
for mod in allowed_modalities:
    configured = RAW_MODALITY_FALLBACKS.get(mod, RAW_MODALITY_FALLBACKS.get(mod.lower(), []))
    MODALITY_FALLBACK_CHAIN[mod] = _normalize_modality_fallback_entries(
        configured,
        mod,
        allowed_modalities,
    )

# -----------------------------------------------------------
# NEW: Global worker data structure for cross-modality tracking
# -----------------------------------------------------------
global_worker_data = {
    'worker_ids': {},  # Map of worker name variations to canonical ID
    # Modality-specific weighted counts and assignments:
    'weighted_counts_per_mod': {mod: {} for mod in allowed_modalities},
    'assignments_per_mod': {mod: {} for mod in allowed_modalities},
    'last_reset_date': None  # Global reset date tracker
}

# -----------------------------------------------------------
# Global state: one "data bucket" per modality.
# -----------------------------------------------------------
modality_data = {}
for mod in allowed_modalities:
    modality_data[mod] = {
        'working_hours_df': None,
        'info_texts': [],
        'total_work_hours': {},
        'worker_modifiers': {},
        'draw_counts': {},
        'skill_counts': {skill: {} for skill in SKILL_COLUMNS},
        'WeightedCounts': {},
        'last_uploaded_filename': f"SBZ_{mod.upper()}.xlsx",  # e.g. SBZ_CT.xlsx
        'default_file_path': os.path.join(app.config['UPLOAD_FOLDER'], f"SBZ_{mod.upper()}.xlsx"),
        'scheduled_file_path': os.path.join(app.config['UPLOAD_FOLDER'], f"SBZ_{mod.upper()}_scheduled.xlsx"),
        'last_reset_date': None
    }


@app.context_processor
def inject_modality_settings():
    return {
        'modalities': MODALITY_SETTINGS,
        'modality_order': allowed_modalities,
        'modality_labels': modality_labels,
        'skill_definitions': SKILL_TEMPLATES,
        'skill_order': SKILL_COLUMNS,
        'skill_labels': {s['name']: s['label'] for s in SKILL_TEMPLATES},
    }


def normalize_modality(modality_value: Optional[str]) -> str:
    if not modality_value:
        return default_modality
    modality_value = modality_value.lower()
    return modality_value if modality_value in allowed_modalities else default_modality


def resolve_modality_from_request() -> str:
    return normalize_modality(request.values.get('modality'))


def normalize_skill(skill_value: Optional[str]) -> str:
    """Validate and normalize skill parameter"""
    if not skill_value:
        return SKILL_COLUMNS[0] if SKILL_COLUMNS else 'Normal'
    # Try exact match first
    if skill_value in SKILL_COLUMNS:
        return skill_value
    # Try case-insensitive match
    skill_value_title = skill_value.title()
    if skill_value_title in SKILL_COLUMNS:
        return skill_value_title
    # Default to first skill
    return SKILL_COLUMNS[0] if SKILL_COLUMNS else 'Normal'


def get_available_modalities_for_skill(skill: str) -> dict:
    """Check which modalities have active workers for this skill"""
    available = {}
    tnow = get_local_berlin_now().time()

    for modality in allowed_modalities:
        d = modality_data[modality]
        if d['working_hours_df'] is not None:
            active_df = d['working_hours_df'][
                (d['working_hours_df']['start_time'] <= tnow) &
                (d['working_hours_df']['end_time'] >= tnow)
            ]
            available[modality] = bool(
                (skill in active_df.columns) and (active_df[skill].sum() > 0)
            )
        else:
            available[modality] = False

    return available

# -----------------------------------------------------------
# TIME / DATE HELPERS (unchanged)
# -----------------------------------------------------------
def get_local_berlin_now() -> datetime:
    tz = pytz.timezone("Europe/Berlin")
    aware_now = datetime.now(tz)
    naive_now = aware_now.replace(tzinfo=None)
    return naive_now

def parse_time_range(time_range: str):
    start_str, end_str = time_range.split('-')
    start_time = datetime.strptime(start_str.strip(), '%H:%M').time()
    end_time   = datetime.strptime(end_str.strip(), '%H:%M').time()
    return start_time, end_time

# -----------------------------------------------------------
# Worker identification helper functions (NEW)
# -----------------------------------------------------------
def get_canonical_worker_id(worker_name):
    """
    Get the canonical worker ID from any name variation.
    If not found, create a new canonical ID.
    """
    if worker_name in global_worker_data['worker_ids']:
        return global_worker_data['worker_ids'][worker_name]
    
    canonical_id = worker_name
    abk_match = worker_name.strip().split('(')
    if len(abk_match) > 1 and ')' in abk_match[1]:
        abbreviation = abk_match[1].split(')')[0].strip()
        canonical_id = abbreviation  # Use abbreviation as canonical ID
    
    global_worker_data['worker_ids'][worker_name] = canonical_id
    return canonical_id

def get_all_workers_by_canonical_id():
    """
    Get a mapping of canonical worker IDs to all their name variations.
    """
    canonical_to_variations = {}
    for name, canonical in global_worker_data['worker_ids'].items():
        if canonical not in canonical_to_variations:
            canonical_to_variations[canonical] = []
        canonical_to_variations[canonical].append(name)
    return canonical_to_variations

# -----------------------------------------------------------
# Medweb CSV Ingestion (Config-Driven)
# -----------------------------------------------------------

def match_mapping_rule(activity_desc: str, rules: list) -> Optional[dict]:
    """Find first matching rule for activity description."""
    if not activity_desc:
        return None
    activity_lower = activity_desc.lower()
    for rule in rules:
        match_str = rule.get('match', '')
        if match_str.lower() in activity_lower:
            return rule
    return None

def apply_roster_overrides(
    base_skills: dict,
    canonical_id: str,
    modality: str,
    worker_roster: dict
) -> dict:
    """Apply per-worker skill overrides from config.yaml worker_skill_roster."""
    if canonical_id not in worker_roster:
        return base_skills.copy()

    final_skills = base_skills.copy()

    # Apply default overrides
    if 'default' in worker_roster[canonical_id]:
        for skill, value in worker_roster[canonical_id]['default'].items():
            if skill in final_skills:
                final_skills[skill] = value

    # Apply modality-specific overrides
    if modality in worker_roster[canonical_id]:
        for skill, value in worker_roster[canonical_id][modality].items():
            if skill in final_skills:
                final_skills[skill] = value

    return final_skills

def compute_time_ranges(
    row: pd.Series,
    rule: dict,
    target_date: datetime,
    config: dict
) -> List[Tuple[time, time]]:
    """
    Compute start/end times based on shift and date.
    Uses shift_times from config.yaml.
    """
    shift_name = rule.get('shift', 'Fruehdienst')
    shift_config = config.get('shift_times', {}).get(shift_name, {})

    if not shift_config:
        # Default fallback
        return [(time(7, 0), time(15, 0))]

    # Check for special days (Friday)
    is_friday = target_date.weekday() == 4

    if is_friday and 'friday' in shift_config:
        time_str = shift_config['friday']
    else:
        time_str = shift_config.get('default', '07:00-15:00')

    # Parse "07:00-15:00"
    try:
        start_str, end_str = time_str.split('-')
        start_time = datetime.strptime(start_str.strip(), '%H:%M').time()
        end_time = datetime.strptime(end_str.strip(), '%H:%M').time()
        return [(start_time, end_time)]
    except Exception:
        return [(time(7, 0), time(15, 0))]

def build_ppl_from_row(row: pd.Series) -> str:
    """Build PPL string from medweb CSV row."""
    name = str(row.get('Name des Mitarbeiters', 'Unknown'))
    code = str(row.get('Code des Mitarbeiters', 'UNK'))
    return f"{name} ({code})"

def get_weekday_name_german(target_date: date) -> str:
    """
    Get German weekday name for a date.

    Returns: Montag, Dienstag, Mittwoch, Donnerstag, Freitag, Samstag, Sonntag
    """
    weekday_names = [
        "Montag", "Dienstag", "Mittwoch", "Donnerstag",
        "Freitag", "Samstag", "Sonntag"
    ]
    return weekday_names[target_date.weekday()]

def parse_duration(duration_str: str) -> timedelta:
    """
    Parse duration string to timedelta.

    Examples:
        "1h30m" → timedelta(hours=1, minutes=30)
        "2h" → timedelta(hours=2)
        "30m" → timedelta(minutes=30)
    """
    hours = 0
    minutes = 0

    # Match hours
    h_match = re.search(r'(\d+)h', duration_str)
    if h_match:
        hours = int(h_match.group(1))

    # Match minutes
    m_match = re.search(r'(\d+)m', duration_str)
    if m_match:
        minutes = int(m_match.group(1))

    return timedelta(hours=hours, minutes=minutes)

def apply_exclusions_to_shifts(
    work_shifts: List[dict],
    exclusions: List[dict],
    target_date: date
) -> List[dict]:
    """
    Apply time exclusions to work shifts (split/truncate as needed).

    Args:
        work_shifts: List of shift dicts with start_time, end_time
        exclusions: List of exclusion dicts with start_time, end_time
        target_date: Date for datetime calculations

    Returns:
        List of modified shift dicts with exclusions applied
    """
    if not exclusions:
        return work_shifts

    result_shifts = []

    for shift in work_shifts:
        shift_start = shift['start_time']
        shift_end = shift['end_time']

        # Convert to datetime for comparison
        shift_start_dt = datetime.combine(target_date, shift_start)
        shift_end_dt = datetime.combine(target_date, shift_end)
        if shift_end_dt < shift_start_dt:
            shift_end_dt += timedelta(days=1)

        # Collect all exclusion periods that overlap with this shift
        overlapping_exclusions = []
        for excl in exclusions:
            excl_start = excl['start_time']
            excl_end = excl['end_time']

            excl_start_dt = datetime.combine(target_date, excl_start)
            excl_end_dt = datetime.combine(target_date, excl_end)
            if excl_end_dt < excl_start_dt:
                excl_end_dt += timedelta(days=1)

            # Check for overlap
            if excl_start_dt < shift_end_dt and excl_end_dt > shift_start_dt:
                overlapping_exclusions.append((excl_start_dt, excl_end_dt))

        if not overlapping_exclusions:
            # No exclusions, keep shift as-is
            result_shifts.append(shift)
            continue

        # Sort exclusions by start time
        overlapping_exclusions.sort(key=lambda x: x[0])

        # Split shift at exclusion boundaries
        current_start = shift_start_dt
        for excl_start_dt, excl_end_dt in overlapping_exclusions:
            # Add segment before exclusion (if any)
            if current_start < excl_start_dt:
                segment_start = current_start.time()
                segment_end = excl_start_dt.time()
                segment_duration = (excl_start_dt - current_start).seconds / 3600

                if segment_duration >= 0.1:  # Minimum 6 minutes
                    result_shifts.append({
                        **shift,
                        'start_time': segment_start,
                        'end_time': segment_end,
                        'shift_duration': segment_duration
                    })

            # Move current_start to after exclusion
            current_start = max(current_start, excl_end_dt)

        # Add remaining segment after all exclusions (if any)
        if current_start < shift_end_dt:
            segment_start = current_start.time()
            segment_end = shift_end_dt.time()
            segment_duration = (shift_end_dt - current_start).seconds / 3600

            if segment_duration >= 0.1:  # Minimum 6 minutes
                result_shifts.append({
                    **shift,
                    'start_time': segment_start,
                    'end_time': segment_end,
                    'shift_duration': segment_duration
                })

    return result_shifts

def build_working_hours_from_medweb(
    csv_path: str,
    target_date: datetime,
    config: dict
) -> Dict[str, pd.DataFrame]:
    """
    Parse medweb CSV and build working_hours_df for each modality.

    Returns:
        {
            'ct': DataFrame(PPL, start_time, end_time, shift_duration, Modifier, Normal, Notfall, ...),
            'mr': DataFrame(...),
            'xray': DataFrame(...)
        }
    """
    # Load CSV
    try:
        medweb_df = pd.read_csv(csv_path, sep=',', encoding='latin1')
    except Exception:
        try:
            medweb_df = pd.read_csv(csv_path, sep=';', encoding='latin1')
        except Exception as e:
            raise ValueError(f"Fehler beim Laden der CSV: {e}")

    # Parse date column
    medweb_df['Datum_parsed'] = pd.to_datetime(
        medweb_df['Datum'], dayfirst=True, errors='coerce'
    ).dt.date

    day_df = medweb_df[medweb_df['Datum_parsed'] == target_date.date()]

    if day_df.empty:
        return {}

    # Get mapping config
    mapping_rules = config.get('medweb_mapping', {}).get('rules', [])
    worker_roster = get_merged_worker_roster(config)

    # Get weekday name for exclusion schedule lookup
    weekday_name = get_weekday_name_german(target_date.date())

    # Prepare data structures
    rows_per_modality = {mod: [] for mod in allowed_modalities}
    exclusions_per_worker = {}  # {canonical_id: [{start_time, end_time, activity}, ...]}

    # FIRST PASS: Process each activity (collect work shifts AND exclusions)
    for _, row in day_df.iterrows():
        activity_desc = str(row.get('Beschreibung der Aktivität', ''))

        # Match rule
        rule = match_mapping_rule(activity_desc, mapping_rules)
        if not rule:
            continue  # Not SBZ-relevant or not mapped

        # Build PPL and get canonical ID (needed for both work and exclusions)
        ppl_str = build_ppl_from_row(row)
        canonical_id = get_canonical_worker_id(ppl_str)

        # Check if this is a time exclusion (board, meeting, etc.)
        if rule.get('exclusion', False):
            # Get schedule for this exclusion
            schedule = rule.get('schedule', {})

            # Check if exclusion applies to today's weekday
            if weekday_name not in schedule:
                # Exclusion doesn't apply today, skip
                continue

            # Parse time range from schedule
            time_range_str = schedule[weekday_name]
            try:
                start_str, end_str = time_range_str.split('-')
                excl_start_time = datetime.strptime(start_str.strip(), '%H:%M').time()
                excl_end_time = datetime.strptime(end_str.strip(), '%H:%M').time()
            except Exception as e:
                selection_logger.warning(
                    f"Could not parse exclusion time range '{time_range_str}' for {activity_desc}: {e}"
                )
                continue

            # Apply prep_time if configured
            prep_time = rule.get('prep_time', {})
            if prep_time:
                # Extend exclusion start backwards (prep before)
                if 'before' in prep_time:
                    prep_before = parse_duration(prep_time['before'])
                    excl_start_dt = datetime.combine(target_date.date(), excl_start_time)
                    excl_start_dt -= prep_before
                    excl_start_time = excl_start_dt.time()

                # Extend exclusion end forwards (cleanup after)
                if 'after' in prep_time:
                    prep_after = parse_duration(prep_time['after'])
                    excl_end_dt = datetime.combine(target_date.date(), excl_end_time)
                    excl_end_dt += prep_after
                    excl_end_time = excl_end_dt.time()

            # Store exclusion for this worker
            if canonical_id not in exclusions_per_worker:
                exclusions_per_worker[canonical_id] = []

            exclusions_per_worker[canonical_id].append({
                'start_time': excl_start_time,
                'end_time': excl_end_time,
                'activity': activity_desc
            })

            selection_logger.info(
                f"Time exclusion for {ppl_str} ({weekday_name}): "
                f"{excl_start_time.strftime('%H:%M')}-{excl_end_time.strftime('%H:%M')} "
                f"({activity_desc})"
            )
            continue  # Don't add to work shifts

        # Normal work activity (not exclusion)
        # Support both single modality and multi-modality (sub-specialty teams)
        target_modalities = []

        if 'modalities' in rule:
            # Multi-modality support (e.g., MSK team across xray, ct, mr)
            raw_modalities = rule['modalities']
            if isinstance(raw_modalities, list):
                target_modalities = [normalize_modality(m) for m in raw_modalities]
            else:
                # Single modality in 'modalities' field (edge case)
                target_modalities = [normalize_modality(raw_modalities)]
        elif 'modality' in rule:
            # Backward compatible: single modality
            target_modalities = [normalize_modality(rule['modality'])]
        else:
            # No modality specified, skip
            continue

        # Filter to only allowed modalities
        target_modalities = [m for m in target_modalities if m in allowed_modalities]

        if not target_modalities:
            continue

        # Base skills from rule (same for all modalities)
        base_skills = {s: 0 for s in SKILL_COLUMNS}
        base_skills.update(rule.get('base_skills', {}))

        # Compute time ranges (same for all modalities)
        time_ranges = compute_time_ranges(row, rule, target_date, config)

        # Create entries for EACH target modality
        for modality in target_modalities:
            # Apply roster overrides (config > worker mapping) per modality
            final_skills = apply_roster_overrides(
                base_skills, canonical_id, modality, worker_roster
            )

            # Add row(s) for each time range in this modality
            for start_time, end_time in time_ranges:
                # Calculate shift duration
                start_dt = datetime.combine(target_date.date(), start_time)
                end_dt = datetime.combine(target_date.date(), end_time)
                if end_dt < start_dt:
                    end_dt += pd.Timedelta(days=1)
                duration_hours = (end_dt - start_dt).seconds / 3600

                rows_per_modality[modality].append({
                    'PPL': ppl_str,
                    'canonical_id': canonical_id,
                    'start_time': start_time,
                    'end_time': end_time,
                    'shift_duration': duration_hours,
                    'Modifier': 1.0,  # Can be extended with modifier_overrides
                    **final_skills
                })

    # SECOND PASS: Apply exclusions to split/truncate shifts
    if exclusions_per_worker:
        selection_logger.info(
            f"Applying time exclusions for {len(exclusions_per_worker)} workers on {weekday_name}"
        )

        for modality in rows_per_modality:
            if not rows_per_modality[modality]:
                continue

            # Group shifts by worker
            shifts_by_worker = {}
            for shift in rows_per_modality[modality]:
                worker_id = shift['canonical_id']
                if worker_id not in shifts_by_worker:
                    shifts_by_worker[worker_id] = []
                shifts_by_worker[worker_id].append(shift)

            # Apply exclusions per worker and rebuild shift list
            new_shifts = []
            for worker_id, worker_shifts in shifts_by_worker.items():
                if worker_id in exclusions_per_worker:
                    # Apply exclusions to this worker's shifts
                    worker_shifts = apply_exclusions_to_shifts(
                        worker_shifts,
                        exclusions_per_worker[worker_id],
                        target_date.date()
                    )
                new_shifts.extend(worker_shifts)

            rows_per_modality[modality] = new_shifts

    # Convert to DataFrames
    result = {}
    for modality, rows in rows_per_modality.items():
        if not rows:
            continue
        df = pd.DataFrame(rows)
        # Remove canonical_id column (used internally only)
        if 'canonical_id' in df.columns:
            df = df.drop(columns=['canonical_id'])
        result[modality] = df

    return result

def get_next_workday(from_date: Optional[datetime] = None) -> datetime:
    """
    Calculate next workday.
    - If Friday: return Monday
    - Otherwise: return next day
    - Skips weekends
    """
    if from_date is None:
        from_date = get_local_berlin_now()

    # If datetime, convert to date
    if hasattr(from_date, 'date'):
        current_date = from_date.date()
    else:
        current_date = from_date

    # Calculate next day
    next_day = current_date + timedelta(days=1)

    # If next day is Saturday (5) or Sunday (6), move to Monday
    while next_day.weekday() >= 5:  # 5=Saturday, 6=Sunday
        next_day += timedelta(days=1)

    return datetime.combine(next_day, time(0, 0))

def auto_preload_job():
    """
    Background job that runs daily at 7:30 AM to preload next workday.
    Uses master CSV if available.
    """
    try:
        if not os.path.exists(MASTER_CSV_PATH):
            selection_logger.warning(f"Auto-preload skipped: No master CSV at {MASTER_CSV_PATH}")
            return

        selection_logger.info(f"Starting auto-preload from {MASTER_CSV_PATH}")

        result = preload_next_workday(MASTER_CSV_PATH, APP_CONFIG)

        if result['success']:
            selection_logger.info(
                f"Auto-preload successful: {result['target_date']}, "
                f"modalities={result['modalities_loaded']}, "
                f"workers={result['total_workers']}"
            )
        else:
            selection_logger.error(f"Auto-preload failed: {result['message']}")

    except Exception as e:
        selection_logger.error(f"Auto-preload exception: {str(e)}", exc_info=True)

def preload_next_workday(csv_path: str, config: dict) -> dict:
    """
    Preload schedule for next workday from medweb CSV.

    Returns:
        {
            'success': bool,
            'target_date': str,
            'modalities_loaded': list,
            'total_workers': int,
            'message': str
        }
    """
    try:
        # Calculate next workday
        next_day = get_next_workday()

        # Parse medweb CSV
        modality_dfs = build_working_hours_from_medweb(
            csv_path,
            next_day,
            config
        )

        if not modality_dfs:
            date_str = next_day.strftime('%Y-%m-%d')
            return {
                'success': False,
                'target_date': date_str,
                'message': f'Keine SBZ-Daten für {date_str} gefunden'
            }

        # Reset all counters and apply to modality_data
        for modality, df in modality_dfs.items():
            d = modality_data[modality]

            # Reset counters
            d['draw_counts'] = {}
            d['skill_counts'] = {skill: {} for skill in SKILL_COLUMNS}
            d['WeightedCounts'] = {}
            global_worker_data['weighted_counts_per_mod'][modality] = {}
            global_worker_data['assignments_per_mod'][modality] = {}

            # Load DataFrame
            d['working_hours_df'] = df

            # Initialize counters
            for worker in df['PPL'].unique():
                d['draw_counts'][worker] = 0
                d['WeightedCounts'][worker] = 0.0
                for skill in SKILL_COLUMNS:
                    if skill not in d['skill_counts']:
                        d['skill_counts'][skill] = {}
                    d['skill_counts'][skill][worker] = 0

            # Set info texts
            d['info_texts'] = []
            d['last_uploaded_filename'] = f"medweb_{next_day.strftime('%Y%m%d')}.csv"

        date_str = next_day.strftime('%Y-%m-%d')
        return {
            'success': True,
            'target_date': date_str,
            'modalities_loaded': list(modality_dfs.keys()),
            'total_workers': sum(len(df) for df in modality_dfs.values()),
            'message': f'Preload erfolgreich für {date_str}'
        }

    except Exception as e:
        return {
            'success': False,
            'target_date': get_next_workday().strftime('%Y-%m-%d'),
            'message': f'Fehler beim Preload: {str(e)}'
        }

def validate_excel_structure(df: pd.DataFrame, required_columns) -> (bool, str):
    # Rename column "PP" to "Privat" if it exists
    if "PP" in df.columns:
        df.rename(columns={"PP": "Privat"}, inplace=True)

    missing_columns = [col for col in required_columns if col not in df.columns]
    if missing_columns:
        return False, f"Fehlende Spalten: {', '.join(missing_columns)}"

    # Example format checks:
    if 'TIME' in df.columns:
        try:
            df['TIME'].apply(parse_time_range)
        except Exception as e:
            return False, f"Falsches Zeitformat in Spalte 'TIME': {str(e)}"

    if 'Modifier' in df.columns:
        try:
            df['Modifier'].astype(str).str.replace(',', '.').astype(float)
        except Exception as e:
            return False, f"Modifier-Spalte ungültiges Format: {str(e)}"

    # Check integer columns for core skills
    for skill in SKILL_COLUMNS:
        if skill in df.columns:
            if not pd.api.types.is_numeric_dtype(df[skill]):
                return False, f"Spalte '{skill}' sollte numerisch sein"

    return True, ""



# -----------------------------------------------------------
# Helper functions to compute global totals across modalities
# -----------------------------------------------------------
def get_global_weighted_count(canonical_id):
    total = 0.0
    for mod in allowed_modalities:
        total += global_worker_data['weighted_counts_per_mod'][mod].get(canonical_id, 0.0)
    return total

def get_global_assignments(canonical_id):
    totals = {skill: 0 for skill in SKILL_COLUMNS}
    totals['total'] = 0
    for mod in allowed_modalities:
        mod_assignments = global_worker_data['assignments_per_mod'][mod].get(canonical_id, {})
        for skill in SKILL_COLUMNS:
            totals[skill] += mod_assignments.get(skill, 0)
        totals['total'] += mod_assignments.get('total', 0)
    return totals

# -----------------------------------------------------------
# Modality-specific work hours & weighted calculations
# -----------------------------------------------------------
def calculate_work_hours_now(current_dt: datetime, modality: str) -> dict:
    d = modality_data[modality]
    if d['working_hours_df'] is None:
        return {}
    df_copy = d['working_hours_df'].copy()
    
    def _calc(row):
        start_dt = datetime.combine(current_dt.date(), row['start_time'])
        end_dt   = datetime.combine(current_dt.date(), row['end_time'])
        if current_dt.time() < row['start_time']:
            return 0.0
        elif current_dt.time() >= row['end_time']:
            return (end_dt - start_dt).total_seconds() / 3600.0
        else:
            return (current_dt - start_dt).total_seconds() / 3600.0

    df_copy['work_hours_now'] = df_copy.apply(_calc, axis=1)
    
    hours_by_canonical = {}
    hours_by_worker = df_copy.groupby('PPL')['work_hours_now'].sum().to_dict()
    
    for worker, hours in hours_by_worker.items():
        canonical_id = get_canonical_worker_id(worker)
        hours_by_canonical[canonical_id] = hours_by_canonical.get(canonical_id, 0) + hours
        
    return hours_by_canonical


# -----------------------------------------------------------
# Data Initialization per modality (based on uploaded Excel)
# -----------------------------------------------------------
def initialize_data(file_path: str, modality: str):
    d = modality_data[modality]
    # Reset all counters for this modality - complete reset
    d['draw_counts'] = {}
    d['skill_counts'] = {skill: {} for skill in SKILL_COLUMNS}
    d['WeightedCounts'] = {}

    # Also reset global counters specific to this modality
    global_worker_data['weighted_counts_per_mod'][modality] = {}
    global_worker_data['assignments_per_mod'][modality] = {}

    with lock:
        try:
            excel_file = pd.ExcelFile(file_path)
            if 'Tabelle1' not in excel_file.sheet_names:
                raise ValueError("Blatt 'Tabelle1' nicht gefunden")

            df = pd.read_excel(excel_file, sheet_name='Tabelle1')

            # Define required columns
            required_columns = ['PPL', 'TIME']
            # Validate Excel structure
            valid, error_msg = validate_excel_structure(df, required_columns)
            if not valid:
                raise ValueError(error_msg)

            # Handle Modifier column
            if 'Modifier' not in df.columns:
                df['Modifier'] = 1.0
            else:
                df['Modifier'] = (
                    df['Modifier']
                    .fillna(1.0)
                    .astype(str)
                    .str.replace(',', '.')
                    .astype(float)
                )

            # Parse TIME into start and end times
            df['start_time'], df['end_time'] = zip(*df['TIME'].map(parse_time_range))

            # Ensure all configured skills exist as integer columns
            for skill in SKILL_COLUMNS:
                if skill not in df.columns:
                    df[skill] = 0
                df[skill] = df[skill].fillna(0).astype(int)

            # Compute shift_duration using the working logic:
            df['shift_duration'] = df.apply(
                lambda row: (
                    datetime.combine(datetime.min, row['end_time']) -
                    datetime.combine(datetime.min, row['start_time'])
                ).total_seconds() / 3600.0,
                axis=1
            )

            # Compute canonical ID for each worker
            df['canonical_id'] = df['PPL'].apply(get_canonical_worker_id)

            # Set column order as desired
            col_order = ['PPL', 'canonical_id', 'Modifier', 'TIME', 'start_time', 'end_time', 'shift_duration']
            skill_cols = [skill for skill in SKILL_COLUMNS if skill in df.columns]
            col_order = col_order[:4] + skill_cols + col_order[4:]
            df = df[[col for col in col_order if col in df.columns]]

            # Save the DataFrame and compute auxiliary data
            d['working_hours_df'] = df
            d['worker_modifiers'] = df.groupby('PPL')['Modifier'].first().to_dict()
            d['total_work_hours'] = df.groupby('PPL')['shift_duration'].sum().to_dict()
            unique_workers = df['PPL'].unique()
            d['draw_counts'] = {w: 0 for w in unique_workers}

            # Initialize skill counts for all workers
            d['skill_counts'] = {}
            for skill in SKILL_COLUMNS:
                if skill in df.columns:
                    d['skill_counts'][skill] = {w: 0 for w in unique_workers}
                else:
                    d['skill_counts'][skill] = {}

            d['WeightedCounts'] = {w: 0.0 for w in unique_workers}

            # Load info texts from Tabelle2 (if available)
            if 'Tabelle2' in excel_file.sheet_names:
                d['info_texts'] = pd.read_excel(excel_file, sheet_name='Tabelle2')['Info'].tolist()
            else:
                d['info_texts'] = []

        except Exception as e:
            error_message = f"Fehler beim Laden der Excel-Datei für Modality '{modality}': {str(e)}"
            selection_logger.error(error_message)
            selection_logger.exception("Stack trace:")
            raise ValueError(error_message)


def quarantine_excel(file_path: str, reason: str) -> Optional[str]:
    """Move a problematic Excel file into uploads/invalid for later inspection."""
    if not file_path or not os.path.exists(file_path):
        return None
    invalid_dir = Path(app.config['UPLOAD_FOLDER']) / 'invalid'
    invalid_dir.mkdir(parents=True, exist_ok=True)
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    original = Path(file_path)
    target = invalid_dir / f"{original.stem}_{timestamp}{original.suffix or '.xlsx'}"
    try:
        shutil.move(str(original), str(target))
        selection_logger.warning(
            "Defekte Excel '%s' nach '%s' verschoben (%s)", file_path, target, reason
        )
        return str(target)
    except OSError as exc:
        selection_logger.warning(
            "Excel '%s' konnte nicht verschoben werden (%s): %s", file_path, reason, exc
        )
        return None


def attempt_initialize_data(
    file_path: str,
    modality: str,
    *,
    remove_on_failure: bool = False,
    context: str = ''
) -> bool:
    """Wrapper around ``initialize_data`` that optionally quarantines bad files."""
    try:
        initialize_data(file_path, modality)
        return True
    except Exception as exc:
        selection_logger.error(
            "Fehler beim Initialisieren der Datei %s für %s (%s): %s",
            file_path,
            modality,
            context or 'runtime',
            exc,
        )
        if remove_on_failure:
            quarantine_excel(file_path, f"{context or 'runtime'}: {exc}")
        return False



# -----------------------------------------------------------
# Active Data Filtering and Weighted-Selection Logic
# -----------------------------------------------------------
def _get_effective_assignment_load(
    worker: str,
    column: str,
    modality: str,
    skill_counts: Optional[dict] = None,
) -> float:
    """Return the worker's current load for the balancer logic.

    A worker may appear "fresh" for the active column even though they have been
    helping another modality via fallback assignments.  To avoid sending the same
    person every time, we combine the local skill counter with the global weighted
    total across all modalities.  The global total already includes the
    weight/modifier math from ``update_global_assignment`` and therefore reflects
    the true amount of recent work performed by the canonical worker ID.
    """

    if skill_counts is None:
        skill_counts = modality_data[modality]['skill_counts'].get(column, {})

    local_count = skill_counts.get(worker, 0)
    canonical_id = get_canonical_worker_id(worker)
    global_weighted_total = get_global_weighted_count(canonical_id)

    # Using max() ensures that any work performed elsewhere (tracked via the
    # weighted total) counts against the minimum-balancer checks.
    return max(float(local_count), float(global_weighted_total))


def _apply_minimum_balancer(filtered_df: pd.DataFrame, column: str, modality: str) -> pd.DataFrame:
    if filtered_df.empty or not BALANCER_SETTINGS.get('enabled', True):
        return filtered_df
    min_required = BALANCER_SETTINGS.get('min_assignments_per_skill', 0)
    if min_required <= 0:
        return filtered_df

    skill_counts = modality_data[modality]['skill_counts'].get(column, {})
    if not skill_counts:
        return filtered_df

    prioritized = filtered_df[
        filtered_df['PPL'].apply(
            lambda worker: _get_effective_assignment_load(worker, column, modality, skill_counts)
            < min_required
        )
    ]
    if prioritized.empty:
        return filtered_df
    return prioritized


def _should_balance_via_fallback(filtered_df: pd.DataFrame, column: str, modality: str) -> bool:
    """
    Check if fallback should be triggered based on workload imbalance.

    Uses work-hour-adjusted ratios (weighted_assignments / hours_worked_so_far)
    to handle overlapping shifts correctly. This ensures imbalance detection
    is consistent with worker selection logic.
    """
    if not isinstance(column, str):
        return False
    if filtered_df.empty or not BALANCER_SETTINGS.get('enabled', True):
        return False
    if not BALANCER_SETTINGS.get('allow_fallback_on_imbalance', True):
        return False

    threshold_pct = float(BALANCER_SETTINGS.get('imbalance_threshold_pct', 0))
    if threshold_pct <= 0:
        return False

    skill_counts = modality_data[modality]['skill_counts'].get(column, {})
    if not skill_counts:
        return False

    # Calculate work hours till now for each worker
    current_dt = get_local_berlin_now()
    hours_map = calculate_work_hours_now(current_dt, modality)

    # Calculate weighted ratios (assignments per hour worked)
    worker_ratios = []
    for worker in filtered_df['PPL'].unique():
        canonical_id = get_canonical_worker_id(worker)
        weighted_assignments = get_global_weighted_count(canonical_id)
        hours_worked = hours_map.get(canonical_id, 0)

        # Skip workers who haven't started their shift yet
        if hours_worked <= 0:
            continue

        ratio = weighted_assignments / hours_worked
        worker_ratios.append(ratio)

    if len(worker_ratios) < 2:
        return False

    max_ratio = max(worker_ratios)
    min_ratio = min(worker_ratios)
    if max_ratio == 0:
        return False

    # Calculate imbalance based on ratios (consistent with worker selection)
    imbalance = (max_ratio - min_ratio) / max_ratio
    return imbalance >= (threshold_pct / 100.0)


def _attempt_column_selection(active_df: pd.DataFrame, column: str, modality: str, is_primary: bool = True):
    """
    Select workers from a specific skill column.

    Skill values:
    - 1 = Active (available for primary and fallback)
    - 0 = Passive (only available in fallback, not for primary requests)
    - -1 = Excluded (has skill but NOT available in fallback)

    Args:
        is_primary: True if selecting for primary skill, False if selecting for fallback
    """
    if column not in active_df.columns:
        return None

    # Filter based on primary vs fallback mode
    if is_primary:
        # Primary selection: only workers with value >= 1 (active workers)
        filtered_df = active_df[active_df[column] >= 1]
    else:
        # Fallback selection: workers with value >= 0 (includes passive, excludes -1)
        filtered_df = active_df[active_df[column] >= 0]

    if filtered_df.empty:
        return None
    balanced_df = _apply_minimum_balancer(filtered_df, column, modality)
    result_df = balanced_df if not balanced_df.empty else filtered_df
    result_df = result_df.copy()
    result_df['__skill_source'] = column
    return result_df


def _try_configured_fallback(active_df: pd.DataFrame, current_column: str, modality: str):
    fallback_chain = BALANCER_FALLBACK_CHAIN.get(current_column, [])
    for fallback in fallback_chain:
        if isinstance(fallback, list):
            combined_frames = []
            for fallback_column in fallback:
                # is_primary=False: fallback mode allows skill value 0
                result = _attempt_column_selection(active_df, fallback_column, modality, is_primary=False)
                if result is not None:
                    combined_frames.append(result)
            if combined_frames:
                merged = pd.concat(combined_frames, ignore_index=True)
                if 'PPL' in merged.columns:
                    merged = merged.drop_duplicates(subset=['PPL'])
                return merged, fallback
        else:
            # is_primary=False: fallback mode allows skill value 0
            result = _attempt_column_selection(active_df, fallback, modality, is_primary=False)
            if result is not None:
                return result, fallback
    return None, current_column


def get_active_df_for_role(
    active_df: pd.DataFrame,
    role: str,
    modality: str,
    allow_fallback: bool = True,
):
    role_map = {
        'normal':  'Normal',
        'notfall': 'Notfall',
        'herz':    'Herz',
        'privat':  'Privat',
        'msk':     'Msk',
        'chest':   'Chest'
    }
    role_lower = role.lower()
    if role_lower not in role_map:
        role_lower = 'normal'
    primary_column = role_map[role_lower]

    # Primary selection: is_primary=True (requires skill value >= 1)
    selection = _attempt_column_selection(active_df, primary_column, modality, is_primary=True)
    if selection is not None:
        filtered_df = selection
        used_column = primary_column
    else:
        filtered_df = None
        used_column = primary_column

    if filtered_df is None and allow_fallback:
        filtered_df, used_column = _try_configured_fallback(active_df, primary_column, modality)

    if filtered_df is None and allow_fallback:
        # Ultimate fallback to Normal: is_primary=False (allows skill value 0)
        normal_df = _attempt_column_selection(active_df, 'Normal', modality, is_primary=False)
        if normal_df is not None:
            filtered_df = normal_df
            used_column = 'Normal'

    if filtered_df is None:
        return active_df.iloc[0:0], primary_column

    if isinstance(used_column, str) and _should_balance_via_fallback(filtered_df, used_column, modality):
        fallback_df, fallback_column = _try_configured_fallback(active_df, used_column, modality)
        if fallback_df is not None:
            filtered_df = fallback_df
            used_column = fallback_column

    return filtered_df, used_column

def _select_worker_for_modality(
    current_dt: datetime,
    role='normal',
    modality=default_modality,
    allow_fallback: bool = True,
):
    d = modality_data[modality]
    if d['working_hours_df'] is None:
        selection_logger.info(f"No working hours data for modality {modality}")
        return None

    tnow = current_dt.time()
    active_df = d['working_hours_df'][
        (d['working_hours_df']['start_time'] <= tnow) &
        (d['working_hours_df']['end_time']   >= tnow)
    ]

    if active_df.empty:
        selection_logger.info(f"No active workers at time {tnow} for modality {modality}")
        return None

    filtered_df, used_column = get_active_df_for_role(
        active_df,
        role,
        modality,
        allow_fallback=allow_fallback,
    )
    
    if filtered_df.empty:
        selection_logger.info(f"No workers found for role {role} (using column {used_column}) at time {tnow}")
        return None

    worker_count = len(filtered_df['PPL'].unique())
    selection_logger.info(
        "Found %s workers for role %s (column %s) in modality %s",
        worker_count,
        role,
        used_column,
        modality,
    )

    hours_map = calculate_work_hours_now(current_dt, modality)

    def weighted_ratio(person):
        canonical_id = get_canonical_worker_id(person)
        h = hours_map.get(canonical_id, 0)
        w = get_global_weighted_count(canonical_id)
        return w / h if h > 0 else w

    available_workers = filtered_df['PPL'].unique()

    if len(available_workers) == 0:
        selection_logger.info(f"No workers available after filtering for {used_column}")
        return None

    best_person = sorted(available_workers, key=lambda p: weighted_ratio(p))[0]
    candidate = filtered_df[filtered_df['PPL'] == best_person].iloc[0].copy()
    candidate['__modality_source'] = modality
    candidate['__selection_ratio'] = weighted_ratio(best_person)
    selection_logger.info(f"Selected candidate: {best_person}")
    return candidate, used_column


def get_next_available_worker(
    current_dt: datetime,
    role='normal',
    modality=default_modality,
    allow_fallback: bool = True,
):
    strategy = BALANCER_SETTINGS.get('fallback_strategy', 'skill_priority')

    if strategy == 'modality_priority':
        return _get_worker_modality_priority(current_dt, role, modality, allow_fallback)
    elif strategy == 'pool_priority':
        return _get_worker_pool_priority(current_dt, role, modality, allow_fallback)
    else:
        return _get_worker_skill_priority(current_dt, role, modality, allow_fallback)


def _get_worker_skill_priority(
    current_dt: datetime,
    role: str,
    modality: str,
    allow_fallback: bool,
):
    """Original behavior: Try all skill fallbacks per modality before moving to next modality."""
    visited = set()

    def _attempt_modality(target_modality: str):
        if target_modality in visited or target_modality not in modality_data:
            return None
        visited.add(target_modality)
        result = _select_worker_for_modality(current_dt, role, target_modality)
        if result is None:
            return None
        candidate, used_column = result
        return candidate, used_column, target_modality

    search_order = [modality] + MODALITY_FALLBACK_CHAIN.get(modality, [])

    for entry in search_order:
        if isinstance(entry, list):
            candidate_pool = []
            for target_modality in entry:
                attempt = _attempt_modality(target_modality)
                if attempt is None:
                    continue
                candidate, used_column, source_modality = attempt
                ratio = candidate.get('__selection_ratio', float('inf'))
                candidate_pool.append((ratio, candidate, used_column, source_modality))
            if candidate_pool:
                ratio, candidate, used_column, source_modality = min(candidate_pool, key=lambda item: item[0])
                if source_modality != modality:
                    selection_logger.info(
                        "Fallback modality %s used for role %s (requested %s)",
                        source_modality,
                        role,
                        modality,
                    )
                return candidate, used_column, source_modality
        else:
            attempt = _attempt_modality(entry)
            if attempt is not None:
                candidate, used_column, source_modality = attempt
                if source_modality != modality:
                    selection_logger.info(
                        "Fallback modality %s used for role %s (requested %s)",
                        source_modality,
                        role,
                        modality,
                    )
                return candidate, used_column, source_modality

    selection_logger.info(
        "No workers available for role %s across modality fallback chain starting with %s",
        role,
        modality,
    )
    return None


def _get_worker_modality_priority(
    current_dt: datetime,
    role: str,
    modality: str,
    allow_fallback: bool,
):
    """New behavior: Try each skill across all modalities before moving to next skill fallback."""

    # Build the skill fallback sequence we'll try
    role_map = {
        'normal':  'Normal',
        'notfall': 'Notfall',
        'herz':    'Herz',
        'privat':  'Privat',
        'msk':     'Msk',
        'chest':   'Chest'
    }
    role_lower = role.lower()
    if role_lower not in role_map:
        role_lower = 'normal'
    primary_skill = role_map[role_lower]

    # Get skill fallback chain for this role
    skill_chain = [primary_skill]
    if allow_fallback:
        configured_fallbacks = BALANCER_FALLBACK_CHAIN.get(primary_skill, [])
        for fallback_entry in configured_fallbacks:
            if isinstance(fallback_entry, list):
                skill_chain.extend(fallback_entry)
            else:
                skill_chain.append(fallback_entry)
        # Add ultimate fallback to Normal if not already there
        if 'Normal' not in skill_chain:
            skill_chain.append('Normal')

    # Build modality search order
    modality_search = [modality] + MODALITY_FALLBACK_CHAIN.get(modality, [])

    # Flatten modality groups
    flat_modality_search = []
    for entry in modality_search:
        if isinstance(entry, list):
            flat_modality_search.extend(entry)
        else:
            flat_modality_search.append(entry)

    # Remove duplicates while preserving order
    seen_modalities = set()
    unique_modality_search = []
    for mod in flat_modality_search:
        if mod not in seen_modalities and mod in modality_data:
            seen_modalities.add(mod)
            unique_modality_search.append(mod)

    # Now iterate: for each skill, try all modalities
    for skill_to_try in skill_chain:
        selection_logger.info(
            "Trying skill %s across modalities %s",
            skill_to_try,
            unique_modality_search,
        )

        # Determine if this is primary or fallback mode
        is_primary_skill = (skill_to_try == primary_skill)

        candidate_pool = []
        for target_modality in unique_modality_search:
            d = modality_data[target_modality]
            if d['working_hours_df'] is None:
                continue

            tnow = current_dt.time()
            active_df = d['working_hours_df'][
                (d['working_hours_df']['start_time'] <= tnow) &
                (d['working_hours_df']['end_time']   >= tnow)
            ]

            if active_df.empty:
                continue

            # Try this specific skill in this specific modality
            # is_primary=True for requested skill, False for fallbacks
            selection = _attempt_column_selection(active_df, skill_to_try, target_modality, is_primary=is_primary_skill)
            if selection is None or selection.empty:
                continue

            # Apply minimum balancer
            balanced_df = _apply_minimum_balancer(selection, skill_to_try, target_modality)
            result_df = balanced_df if not balanced_df.empty else selection

            # Select best worker from this modality
            hours_map = calculate_work_hours_now(current_dt, target_modality)

            def weighted_ratio(person):
                canonical_id = get_canonical_worker_id(person)
                h = hours_map.get(canonical_id, 0)
                w = get_global_weighted_count(canonical_id)
                return w / h if h > 0 else w

            available_workers = result_df['PPL'].unique()
            if len(available_workers) == 0:
                continue

            best_person = sorted(available_workers, key=lambda p: weighted_ratio(p))[0]
            candidate = result_df[result_df['PPL'] == best_person].iloc[0].copy()
            candidate['__modality_source'] = target_modality
            candidate['__selection_ratio'] = weighted_ratio(best_person)

            ratio = candidate.get('__selection_ratio', float('inf'))
            candidate_pool.append((ratio, candidate, skill_to_try, target_modality))

        # If we found candidates for this skill, return the best one
        if candidate_pool:
            ratio, candidate, used_skill, source_modality = min(candidate_pool, key=lambda item: item[0])

            if source_modality != modality or used_skill != primary_skill:
                selection_logger.info(
                    "Fallback used: skill=%s, modality=%s (requested: skill=%s, modality=%s)",
                    used_skill,
                    source_modality,
                    primary_skill,
                    modality,
                )

            return candidate, used_skill, source_modality

    selection_logger.info(
        "No workers available for role %s across all skill and modality fallbacks (modality_priority mode)",
        role,
    )
    return None


def _get_worker_pool_priority(
    current_dt: datetime,
    role: str,
    modality: str,
    allow_fallback: bool,
):
    """Pool-based approach: Collect ALL possible (skill, modality) combinations and pick the globally best one."""

    # Build the skill fallback sequence
    role_map = {
        'normal':  'Normal',
        'notfall': 'Notfall',
        'herz':    'Herz',
        'privat':  'Privat',
        'msk':     'Msk',
        'chest':   'Chest'
    }
    role_lower = role.lower()
    if role_lower not in role_map:
        role_lower = 'normal'
    primary_skill = role_map[role_lower]

    # Get skill fallback chain
    skill_chain = [primary_skill]
    if allow_fallback:
        configured_fallbacks = BALANCER_FALLBACK_CHAIN.get(primary_skill, [])
        for fallback_entry in configured_fallbacks:
            if isinstance(fallback_entry, list):
                skill_chain.extend(fallback_entry)
            else:
                skill_chain.append(fallback_entry)
        # Add ultimate fallback to Normal if not already there
        if 'Normal' not in skill_chain:
            skill_chain.append('Normal')

    # Build modality search order
    modality_search = [modality] + MODALITY_FALLBACK_CHAIN.get(modality, [])

    # Flatten modality groups
    flat_modality_search = []
    for entry in modality_search:
        if isinstance(entry, list):
            flat_modality_search.extend(entry)
        else:
            flat_modality_search.append(entry)

    # Remove duplicates while preserving order
    seen_modalities = set()
    unique_modality_search = []
    for mod in flat_modality_search:
        if mod not in seen_modalities and mod in modality_data:
            seen_modalities.add(mod)
            unique_modality_search.append(mod)

    # Build the complete pool: all (skill, modality) combinations
    selection_logger.info(
        "Building candidate pool for role %s: skills=%s, modalities=%s (pool_priority mode)",
        role,
        skill_chain,
        unique_modality_search,
    )

    candidate_pool = []

    for skill_to_try in skill_chain:
        # Determine if this is primary or fallback mode
        is_primary_skill = (skill_to_try == primary_skill)

        for target_modality in unique_modality_search:
            d = modality_data[target_modality]
            if d['working_hours_df'] is None:
                continue

            tnow = current_dt.time()
            active_df = d['working_hours_df'][
                (d['working_hours_df']['start_time'] <= tnow) &
                (d['working_hours_df']['end_time']   >= tnow)
            ]

            if active_df.empty:
                continue

            # Try this specific skill in this specific modality
            # is_primary=True for requested skill, False for fallbacks
            selection = _attempt_column_selection(active_df, skill_to_try, target_modality, is_primary=is_primary_skill)
            if selection is None or selection.empty:
                continue

            # Apply minimum balancer
            balanced_df = _apply_minimum_balancer(selection, skill_to_try, target_modality)
            result_df = balanced_df if not balanced_df.empty else selection

            # Select best worker from this (skill, modality) combination
            hours_map = calculate_work_hours_now(current_dt, target_modality)

            def weighted_ratio(person):
                canonical_id = get_canonical_worker_id(person)
                h = hours_map.get(canonical_id, 0)
                w = get_global_weighted_count(canonical_id)
                return w / h if h > 0 else w

            available_workers = result_df['PPL'].unique()
            if len(available_workers) == 0:
                continue

            # Get the best worker for this specific (skill, modality) combination
            best_person = sorted(available_workers, key=lambda p: weighted_ratio(p))[0]
            candidate = result_df[result_df['PPL'] == best_person].iloc[0].copy()
            candidate['__modality_source'] = target_modality
            candidate['__selection_ratio'] = weighted_ratio(best_person)

            ratio = candidate.get('__selection_ratio', float('inf'))
            candidate_pool.append((ratio, candidate, skill_to_try, target_modality))

    # Now select the globally best candidate from the entire pool
    if not candidate_pool:
        selection_logger.info(
            "No workers available for role %s across all skill and modality combinations (pool_priority mode)",
            role,
        )
        return None

    # Sort by ratio and pick the best
    ratio, candidate, used_skill, source_modality = min(candidate_pool, key=lambda item: item[0])

    selection_logger.info(
        "Selected from pool of %d candidates: skill=%s, modality=%s, person=%s, ratio=%.4f (requested: skill=%s, modality=%s)",
        len(candidate_pool),
        used_skill,
        source_modality,
        candidate.get('PPL', 'unknown'),
        ratio,
        primary_skill,
        modality,
    )

    if source_modality != modality or used_skill != primary_skill:
        selection_logger.info(
            "Fallback used: skill=%s, modality=%s (requested: skill=%s, modality=%s)",
            used_skill,
            source_modality,
            primary_skill,
            modality,
        )

    return candidate, used_skill, source_modality

# -----------------------------------------------------------
# Daily Reset: check (for every modality) at >= 07:30
# -----------------------------------------------------------
def check_and_perform_daily_reset():
    now = get_local_berlin_now()
    today = now.date()
    
    if global_worker_data['last_reset_date'] != today and now.time() >= time(7, 30):
        should_reset_global = any(
            os.path.exists(modality_data[mod]['scheduled_file_path']) 
            for mod in allowed_modalities
        )
        if should_reset_global:
            global_worker_data['last_reset_date'] = today
            selection_logger.info("Performed global reset based on modality scheduled uploads.")
        
    for mod, d in modality_data.items():
        if d['last_reset_date'] == today:
            continue
        if now.time() >= time(7, 30):
            if os.path.exists(d['scheduled_file_path']):
                # Reset all counters for this modality before initializing new data
                d['draw_counts'] = {}
                d['skill_counts'] = {skill: {} for skill in SKILL_COLUMNS}
                d['WeightedCounts'] = {}

                context = f"daily reset {mod.upper()}"
                success = attempt_initialize_data(
                    d['scheduled_file_path'],
                    mod,
                    remove_on_failure=True,
                    context=context,
                )
                if success:
                    backup_dir = os.path.join(app.config['UPLOAD_FOLDER'], "backups")
                    os.makedirs(backup_dir, exist_ok=True)
                    backup_file = os.path.join(backup_dir, os.path.basename(d['scheduled_file_path']))
                    try:
                        shutil.move(d['scheduled_file_path'], backup_file)
                    except OSError as exc:
                        selection_logger.warning(
                            "Scheduled Datei %s konnte nicht verschoben werden: %s",
                            d['scheduled_file_path'],
                            exc,
                        )
                    else:
                        selection_logger.info(
                            "Scheduled daily file loaded and moved to backup for modality %s.",
                            mod,
                        )
                    backup_dataframe(mod)
                    selection_logger.info(
                        "Live-backup updated for modality %s after daily reset.",
                        mod,
                    )
                else:
                    selection_logger.warning(
                        "Scheduled file for %s war defekt und wurde entfernt.",
                        mod,
                    )

            else:
                selection_logger.info(f"No scheduled file found for modality {mod}. Keeping old data.")
            d['last_reset_date'] = today
            global_worker_data['weighted_counts_per_mod'][mod] = {}
            global_worker_data['assignments_per_mod'][mod] = {}
            
@app.before_request
def before_request():
    check_and_perform_daily_reset()

# -----------------------------------------------------------
# Helper for low-duplication global update
# -----------------------------------------------------------
def _get_or_create_assignments(modality: str, canonical_id: str) -> dict:
    assignments = global_worker_data['assignments_per_mod'][modality]
    if canonical_id not in assignments:
        assignments[canonical_id] = {skill: 0 for skill in SKILL_COLUMNS}
        assignments[canonical_id]['total'] = 0
    return assignments[canonical_id]

def update_global_assignment(person: str, role: str, modality: str) -> str:
    canonical_id = get_canonical_worker_id(person)
    # Get the modifier (default 1.0). Values > 1 mean more work, < 1 mean less work.
    modifier = modality_data[modality]['worker_modifiers'].get(person, 1.0)
    modifier = _coerce_float(modifier, 1.0)
    weight = skill_weights.get(role, 1.0) * modifier * modality_factors.get(modality, 1.0)

    global_worker_data['weighted_counts_per_mod'][modality][canonical_id] = \
        global_worker_data['weighted_counts_per_mod'][modality].get(canonical_id, 0.0) + weight

    assignments = _get_or_create_assignments(modality, canonical_id)
    assignments[role] += 1
    assignments['total'] += 1

    return canonical_id

# -----------------------------------------------------------
# Helper: Live Backup of DataFrame
# -----------------------------------------------------------
def backup_dataframe(modality: str):
    """
    Writes the current working_hours_df for the given modality to a live backup Excel file.
    The backup file will include:
      - "Tabelle1": containing the working_hours_df data without extra columns.
      - "Tabelle2": containing the info_texts (if available).
      
    This version removes the columns 'start_time', 'end_time', and 'shift_duration'.
    """
    d = modality_data[modality]
    if d['working_hours_df'] is not None:
        backup_dir = os.path.join(app.config['UPLOAD_FOLDER'], "backups")
        os.makedirs(backup_dir, exist_ok=True)
        backup_file = os.path.join(backup_dir, f"SBZ_{modality.upper()}_live.xlsx")
        try:
            # Remove unwanted columns from backup
            cols_to_backup = [col for col in d['working_hours_df'].columns
                              if col not in ['start_time', 'end_time', 'shift_duration']]
            df_backup = d['working_hours_df'][cols_to_backup].copy()
            
            with pd.ExcelWriter(backup_file, engine='openpyxl') as writer:
                # Write the filtered DataFrame into sheet "Tabelle1"
                df_backup.to_excel(writer, sheet_name='Tabelle1', index=False)
                # If info_texts are available, write them into sheet "Tabelle2"
                if d.get('info_texts'):
                    df_info = pd.DataFrame({'Info': d['info_texts']})
                    df_info.to_excel(writer, sheet_name='Tabelle2', index=False)
            selection_logger.info(f"Live backup updated for modality {modality} at {backup_file}")
        except Exception as e:
            selection_logger.info(f"Error backing up DataFrame for modality {modality}: {e}")


# -----------------------------------------------------------
# Startup: initialize each modality – zuerst das aktuelle Live-Backup, dann das Default-File
# -----------------------------------------------------------
for mod, d in modality_data.items():
    backup_dir  = os.path.join(app.config['UPLOAD_FOLDER'], "backups")
    backup_path = os.path.join(backup_dir, f"SBZ_{mod.upper()}_live.xlsx")

    loaded = False

    if os.path.exists(backup_path):
        if attempt_initialize_data(
            backup_path,
            mod,
            remove_on_failure=True,
            context=f"startup backup {mod.upper()}",
        ):
            selection_logger.info(
                f"Initialized {mod.upper()} modality from live-backup: {backup_path}"
            )
            loaded = True
        else:
            selection_logger.info(
                f"Live-backup für {mod.upper()} war defekt und wurde entfernt."
            )

    if not loaded and os.path.exists(d['default_file_path']):
        if attempt_initialize_data(
            d['default_file_path'],
            mod,
            remove_on_failure=True,
            context=f"startup default {mod.upper()}",
        ):
            selection_logger.info(
                f"Initialized {mod.upper()} modality from default file: {d['default_file_path']}"
            )
            loaded = True
        else:
            selection_logger.info(
                f"Default-File für {mod.upper()} war defekt und wurde entfernt."
            )

    if not loaded:
        selection_logger.info(
            f"Kein verwendbares File für {mod.upper()} gefunden – starte leer."
        )
        d['working_hours_df'] = None
        d['info_texts'] = []
        d['total_work_hours'] = {}
        d['worker_modifiers'] = {}
        d['draw_counts'] = {}
        d['skill_counts'] = {skill: {} for skill in SKILL_COLUMNS}
        d['WeightedCounts'] = {}
        d['last_reset_date'] = None

# -----------------------------------------------------------
# Routes
# -----------------------------------------------------------
@app.route('/')
def index():
    modality = resolve_modality_from_request()
    d = modality_data[modality]

    # Determine available specialties based on currently active working hours
    available_specialties = {SKILL_SLUG_MAP[skill]: False for skill in SKILL_COLUMNS}
    if d['working_hours_df'] is not None:
        tnow = get_local_berlin_now().time()
        active_df = d['working_hours_df'][
            (d['working_hours_df']['start_time'] <= tnow) &
            (d['working_hours_df']['end_time'] >= tnow)
        ]
        for skill in SKILL_COLUMNS:
            slug = SKILL_SLUG_MAP[skill]
            available_specialties[slug] = bool(
                (skill in active_df.columns) and (active_df[skill].sum() > 0)
            )

    for entry in SKILL_TEMPLATES:
        if entry['always_visible']:
            available_specialties[entry['slug']] = True

    return render_template(
        'index.html',
        available_specialties=available_specialties,
        info_texts=d.get('info_texts', []),
        modality=modality
    )


@app.route('/by-skill')
def index_by_skill():
    """
    Skill-based view: navigate by skill, see all modalities as buttons
    """
    skill = request.args.get('skill', SKILL_COLUMNS[0] if SKILL_COLUMNS else 'Normal')
    skill = normalize_skill(skill)

    # Determine available modalities for this skill (check working hours)
    available_modalities_dict = get_available_modalities_for_skill(skill)

    # Get info texts from first modality (they're typically the same)
    info_texts = []
    if allowed_modalities:
        first_modality = allowed_modalities[0]
        info_texts = modality_data[first_modality].get('info_texts', [])

    return render_template(
        'index_by_skill.html',
        skill=skill,
        available_modalities=available_modalities_dict,
        info_texts=info_texts
    )


def get_admin_password():
    try:
        with open("config.yaml", "r") as f:
            config = yaml.safe_load(f)
        return config.get("admin_password", "")
    except Exception as e:
        selection_logger.info("Error loading config.yaml:", e)
        return ""

def run_operational_checks(context: str = 'unknown', force: bool = False) -> dict:
    """
    Run operational readiness checks for the system.

    Args:
        context: Context string describing where checks are being run from
        force: Force re-run even if cached (currently always runs)

    Returns:
        Dictionary with:
        - results: list of check results (name, status, detail)
        - context: the context string
        - timestamp: ISO format timestamp
    """
    results = []
    now = get_local_berlin_now().isoformat()

    # Check 1: Config file exists and is readable
    try:
        with open("config.yaml", "r") as f:
            config = yaml.safe_load(f)
        results.append({
            'name': 'Config File',
            'status': 'OK',
            'detail': 'config.yaml is readable and valid YAML'
        })
    except Exception as e:
        results.append({
            'name': 'Config File',
            'status': 'ERROR',
            'detail': f'Failed to load config.yaml: {str(e)}'
        })

    # Check 2: Admin password is set (not default)
    try:
        admin_pw = get_admin_password()
        if not admin_pw:
            results.append({
                'name': 'Admin Password',
                'status': 'WARNING',
                'detail': 'Admin password is not set in config.yaml'
            })
        elif admin_pw == 'change_pw_for_live':
            results.append({
                'name': 'Admin Password',
                'status': 'WARNING',
                'detail': 'Admin password is still set to default value - change for production!'
            })
        else:
            results.append({
                'name': 'Admin Password',
                'status': 'OK',
                'detail': 'Admin password is configured'
            })
    except Exception as e:
        results.append({
            'name': 'Admin Password',
            'status': 'ERROR',
            'detail': f'Failed to check admin password: {str(e)}'
        })

    # Check 3: Upload folder exists and is writable
    try:
        upload_folder = app.config.get('UPLOAD_FOLDER', 'uploads')
        if not os.path.exists(upload_folder):
            results.append({
                'name': 'Upload Folder',
                'status': 'WARNING',
                'detail': f'Upload folder "{upload_folder}" does not exist (will be created on upload)'
            })
        elif not os.access(upload_folder, os.W_OK):
            results.append({
                'name': 'Upload Folder',
                'status': 'ERROR',
                'detail': f'Upload folder "{upload_folder}" is not writable'
            })
        else:
            file_count = len([f for f in os.listdir(upload_folder) if f.endswith('.xlsx')])
            results.append({
                'name': 'Upload Folder',
                'status': 'OK',
                'detail': f'Upload folder "{upload_folder}" is writable ({file_count} Excel files found)'
            })
    except Exception as e:
        results.append({
            'name': 'Upload Folder',
            'status': 'ERROR',
            'detail': f'Failed to check upload folder: {str(e)}'
        })

    # Check 4: Modalities configured
    try:
        modality_count = len(allowed_modalities)
        if modality_count == 0:
            results.append({
                'name': 'Modalities',
                'status': 'ERROR',
                'detail': 'No modalities configured in config.yaml'
            })
        else:
            results.append({
                'name': 'Modalities',
                'status': 'OK',
                'detail': f'{modality_count} modalities configured: {", ".join(allowed_modalities)}'
            })
    except Exception as e:
        results.append({
            'name': 'Modalities',
            'status': 'ERROR',
            'detail': f'Failed to check modalities: {str(e)}'
        })

    # Check 5: Skills configured
    try:
        skill_count = len(SKILL_COLUMNS)
        if skill_count == 0:
            results.append({
                'name': 'Skills',
                'status': 'ERROR',
                'detail': 'No skills configured in config.yaml'
            })
        else:
            results.append({
                'name': 'Skills',
                'status': 'OK',
                'detail': f'{skill_count} skills configured: {", ".join(SKILL_COLUMNS)}'
            })
    except Exception as e:
        results.append({
            'name': 'Skills',
            'status': 'ERROR',
            'detail': f'Failed to check skills: {str(e)}'
        })

    # Check 6: Worker data loaded
    try:
        total_workers = 0
        for mod in allowed_modalities:
            d = modality_data.get(mod, {})
            if d.get('working_hours_df') is not None:
                total_workers += len(d['working_hours_df']['PPL'].unique())

        if total_workers == 0:
            results.append({
                'name': 'Worker Data',
                'status': 'WARNING',
                'detail': 'No worker data loaded - upload an Excel file to get started'
            })
        else:
            results.append({
                'name': 'Worker Data',
                'status': 'OK',
                'detail': f'{total_workers} workers loaded across all modalities'
            })
    except Exception as e:
        results.append({
            'name': 'Worker Data',
            'status': 'ERROR',
            'detail': f'Failed to check worker data: {str(e)}'
        })

    return {
        'results': results,
        'context': context,
        'timestamp': now
    }

# --- Create a decorator to protect admin routes:
def admin_required(f):
    @wraps(f)
    def decorated(*args, **kwargs):
        if not session.get('admin_logged_in'):
            # redirect to login page and pass current modality if needed
            modality = resolve_modality_from_request()
            return redirect(url_for('login', modality=modality))
        return f(*args, **kwargs)
    return decorated

# --- Add a login route:
@app.route('/login', methods=['GET', 'POST'])
def login():
    modality = resolve_modality_from_request()
    error = None
    if request.method == 'POST':
        pw = request.form.get('password', '')
        if pw == get_admin_password():
            session['admin_logged_in'] = True
            return redirect(url_for('upload_file', modality=modality))
        else:
            error = "Falsches Passwort"
    return render_template("login.html", error=error, modality=modality)


@app.route('/logout')
def logout():
    session.pop('admin_logged_in', None)
    modality = resolve_modality_from_request()
    return redirect(url_for('index', modality=modality))


@app.route('/upload', methods=['GET', 'POST'])
@admin_required
def upload_file():
    """
    Medweb CSV upload route (config-driven).
    Replaces old Excel per-modality upload.
    """
    if request.method == 'POST':
        if 'file' not in request.files:
            return jsonify({"error": "Keine Datei ausgewählt"}), 400
        file = request.files['file']
        if file.filename == '':
            return jsonify({"error": "Keine Datei ausgewählt"}), 400
        if not file.filename.lower().endswith('.csv'):
            return jsonify({"error": "Ungültiger Dateityp. Bitte eine CSV-Datei hochladen."}), 400

        target_date_str = request.form.get('target_date')
        if not target_date_str:
            return jsonify({"error": "Bitte Zieldatum angeben"}), 400

        try:
            target_date = datetime.strptime(target_date_str, '%Y-%m-%d')
        except Exception:
            return jsonify({"error": "Ungültiges Datumsformat"}), 400

        # Save CSV temporarily
        csv_path = os.path.join(app.config['UPLOAD_FOLDER'], 'medweb_temp.csv')
        try:
            file.save(csv_path)

            # Parse medweb CSV
            modality_dfs = build_working_hours_from_medweb(
                csv_path,
                target_date,
                APP_CONFIG
            )

            if not modality_dfs:
                return jsonify({"error": f"Keine SBZ-Daten für {target_date.strftime('%Y-%m-%d')} gefunden"}), 400

            # Reset all counters and apply to modality_data
            for modality, df in modality_dfs.items():
                d = modality_data[modality]

                # Reset counters
                d['draw_counts'] = {}
                d['skill_counts'] = {skill: {} for skill in SKILL_COLUMNS}
                d['WeightedCounts'] = {}
                global_worker_data['weighted_counts_per_mod'][modality] = {}
                global_worker_data['assignments_per_mod'][modality] = {}

                # Load DataFrame
                d['working_hours_df'] = df

                # Initialize counters
                for worker in df['PPL'].unique():
                    d['draw_counts'][worker] = 0
                    d['WeightedCounts'][worker] = 0.0
                    for skill in SKILL_COLUMNS:
                        if skill not in d['skill_counts']:
                            d['skill_counts'][skill] = {}
                        d['skill_counts'][skill][worker] = 0

                # Set info texts (empty for now, can be extended)
                d['info_texts'] = []
                d['last_uploaded_filename'] = f"medweb_{target_date.strftime('%Y%m%d')}.csv"

            # Save to master CSV for auto-preload
            shutil.copy2(csv_path, MASTER_CSV_PATH)
            selection_logger.info(f"Master CSV updated: {MASTER_CSV_PATH}")

            # Clean up temp file
            os.remove(csv_path)

            return jsonify({
                "success": True,
                "message": f"Medweb CSV erfolgreich geladen für {target_date.strftime('%Y-%m-%d')}",
                "modalities_loaded": list(modality_dfs.keys()),
                "total_workers": sum(len(df) for df in modality_dfs.values())
            })

        except Exception as e:
            if os.path.exists(csv_path):
                os.remove(csv_path)
            return jsonify({"error": f"Fehler beim Verarbeiten der CSV: {str(e)}"}), 500

    # GET method: Show upload page with stats
    # Get first modality for display
    modality = resolve_modality_from_request()
    d = modality_data[modality]

    # Compute combined stats across all modalities
    all_worker_names = set()
    combined_skill_counts = {skill: {} for skill in SKILL_COLUMNS}

    for mod_key in allowed_modalities:
        mod_d = modality_data[mod_key]
        for skill in SKILL_COLUMNS:
            for worker, count in mod_d['skill_counts'].get(skill, {}).items():
                all_worker_names.add(worker)
                if worker not in combined_skill_counts[skill]:
                    combined_skill_counts[skill][worker] = 0
                combined_skill_counts[skill][worker] += count

    # Compute sum counts and global counts
    sum_counts = {}
    global_counts = {}
    global_weighted_counts = {}
    for worker in all_worker_names:
        total = sum(combined_skill_counts[skill].get(worker, 0) for skill in SKILL_COLUMNS)
        sum_counts[worker] = total

        canonical = get_canonical_worker_id(worker)
        global_counts[worker] = get_global_assignments(canonical)
        global_weighted_counts[worker] = get_global_weighted_count(canonical)

    # Build combined stats table
    combined_workers = sorted(all_worker_names)
    modality_stats = {}
    for worker in combined_workers:
        modality_stats[worker] = {
            skill: combined_skill_counts[skill].get(worker, 0)
            for skill in SKILL_COLUMNS
        }
        modality_stats[worker]['total'] = sum_counts.get(worker, 0)

    # Debug info from first loaded modality
    debug_info = (
        d['working_hours_df'].to_html(index=True)
        if d['working_hours_df'] is not None else "Keine Daten verfügbar"
    )

    # Run operational checks
    checks = run_operational_checks('admin_view', force=True)

    return render_template(
        'upload.html',
        debug_info=debug_info,
        modality=modality,
        skill_counts=combined_skill_counts,
        sum_counts=sum_counts,
        global_counts=global_counts,
        global_weighted_counts=global_weighted_counts,
        combined_workers=combined_workers,
        modality_stats=modality_stats,
        operational_checks=checks,
    )


@app.route('/preload-next-day', methods=['POST'])
@admin_required
def preload_next_day():
    """
    Preload schedule for next workday from stored medweb CSV.
    - Friday → loads Monday
    - Other days → loads tomorrow
    """
    if 'file' not in request.files:
        return jsonify({"error": "Keine Datei ausgewählt"}), 400

    file = request.files['file']
    if file.filename == '':
        return jsonify({"error": "Keine Datei ausgewählt"}), 400

    if not file.filename.lower().endswith('.csv'):
        return jsonify({"error": "Ungültiger Dateityp. Bitte eine CSV-Datei hochladen."}), 400

    # Save CSV
    csv_path = os.path.join(app.config['UPLOAD_FOLDER'], 'medweb_preload.csv')
    try:
        file.save(csv_path)

        # Preload next workday
        result = preload_next_workday(csv_path, APP_CONFIG)

        # Save to master CSV for future auto-preload
        if result['success']:
            shutil.copy2(csv_path, MASTER_CSV_PATH)
            selection_logger.info(f"Master CSV updated via preload: {MASTER_CSV_PATH}")

        # Clean up temp file
        if os.path.exists(csv_path):
            os.remove(csv_path)

        if result['success']:
            return jsonify(result)
        else:
            return jsonify(result), 400

    except Exception as e:
        if os.path.exists(csv_path):
            os.remove(csv_path)
        return jsonify({"error": f"Fehler beim Preload: {str(e)}"}), 500


@app.route('/force-refresh-today', methods=['POST'])
@admin_required
def force_refresh_today():
    """
    Force refresh current day's schedule from new CSV.
    Overwrites all current data and resets counters.
    Use for emergency changes during the day.
    """
    if 'file' not in request.files:
        return jsonify({"error": "Keine Datei ausgewählt"}), 400

    file = request.files['file']
    if file.filename == '':
        return jsonify({"error": "Keine Datei ausgewählt"}), 400

    if not file.filename.lower().endswith('.csv'):
        return jsonify({"error": "Ungültiger Dateityp. Bitte eine CSV-Datei hochladen."}), 400

    # Save CSV temporarily
    csv_path = os.path.join(app.config['UPLOAD_FOLDER'], 'medweb_force_refresh.csv')
    try:
        file.save(csv_path)

        # Use TODAY's date
        target_date = get_local_berlin_now()

        # Parse medweb CSV
        modality_dfs = build_working_hours_from_medweb(
            csv_path,
            target_date,
            APP_CONFIG
        )

        if not modality_dfs:
            return jsonify({"error": f"Keine SBZ-Daten für {target_date.strftime('%Y-%m-%d')} gefunden"}), 400

        # CRITICAL: Reset ALL counters and apply to modality_data
        for modality, df in modality_dfs.items():
            d = modality_data[modality]

            # Reset counters (this loses all assignment history!)
            d['draw_counts'] = {}
            d['skill_counts'] = {skill: {} for skill in SKILL_COLUMNS}
            d['WeightedCounts'] = {}
            global_worker_data['weighted_counts_per_mod'][modality] = {}
            global_worker_data['assignments_per_mod'][modality] = {}

            # Load DataFrame
            d['working_hours_df'] = df

            # Initialize counters
            for worker in df['PPL'].unique():
                d['draw_counts'][worker] = 0
                d['WeightedCounts'][worker] = 0.0
                for skill in SKILL_COLUMNS:
                    if skill not in d['skill_counts']:
                        d['skill_counts'][skill] = {}
                    d['skill_counts'][skill][worker] = 0

            # Set info texts
            d['info_texts'] = []
            d['last_uploaded_filename'] = f"force_refresh_{target_date.strftime('%Y%m%d')}.csv"

        # Save to master CSV for future auto-preload
        shutil.copy2(csv_path, MASTER_CSV_PATH)
        selection_logger.warning(
            f"Force refresh executed for {target_date.strftime('%Y-%m-%d')}, "
            f"all counters reset! Modalities: {list(modality_dfs.keys())}"
        )

        # Clean up temp file
        os.remove(csv_path)

        return jsonify({
            "success": True,
            "message": f"Force Refresh erfolgreich für {target_date.strftime('%Y-%m-%d')} (ALLE Zählerstände wurden zurückgesetzt!)",
            "modalities_loaded": list(modality_dfs.keys()),
            "total_workers": sum(len(df) for df in modality_dfs.values()),
            "warning": "Alle bisherigen Zuteilungen wurden gelöscht!"
        })

    except Exception as e:
        if os.path.exists(csv_path):
            os.remove(csv_path)
        return jsonify({"error": f"Fehler beim Force Refresh: {str(e)}"}), 500


@app.route('/prep-next-day')
@admin_required
def prep_next_day():
    """
    Next day prep/edit page.
    Shows editable table for tomorrow's schedule.
    Can be used for both normal prep and force refresh scenarios.
    """
    next_day = get_next_workday()

    return render_template(
        'prep_next_day.html',
        target_date=next_day.strftime('%Y-%m-%d'),
        target_date_german=next_day.strftime('%d.%m.%Y'),
        is_next_day=True
    )


@app.route('/api/prep-next-day/data', methods=['GET'])
@admin_required
def get_prep_data():
    """
    Get current working_hours_df data for all modalities.
    Returns data in format suitable for edit table.
    """
    result = {}

    for modality in allowed_modalities:
        d = modality_data[modality]
        df = d.get('working_hours_df')

        if df is not None and not df.empty:
            # Convert DataFrame to list of dicts for JSON
            data = []
            for idx, row in df.iterrows():
                worker_data = {
                    'row_index': int(idx),
                    'PPL': row['PPL'],
                    'start_time': row['start_time'].strftime('%H:%M') if pd.notnull(row['start_time']) else '',
                    'end_time': row['end_time'].strftime('%H:%M') if pd.notnull(row['end_time']) else '',
                    'Modifier': float(row.get('Modifier', 1.0)),
                }

                # Add all skill columns
                for skill in SKILL_COLUMNS:
                    worker_data[skill] = int(row.get(skill, 0))

                data.append(worker_data)

            result[modality] = data
        else:
            result[modality] = []

    return jsonify(result)


@app.route('/api/prep-next-day/update-row', methods=['POST'])
@admin_required
def update_prep_row():
    """
    Update a single worker row in working_hours_df.
    """
    try:
        data = request.json
        modality = data.get('modality')
        row_index = data.get('row_index')
        updates = data.get('updates', {})

        if modality not in modality_data:
            return jsonify({'error': 'Invalid modality'}), 400

        df = modality_data[modality]['working_hours_df']

        if df is None or row_index >= len(df):
            return jsonify({'error': 'Invalid row index'}), 400

        # Apply updates
        for col, value in updates.items():
            if col in ['start_time', 'end_time']:
                # Parse time string
                try:
                    df.at[row_index, col] = datetime.strptime(value, '%H:%M').time()
                except:
                    return jsonify({'error': f'Invalid time format for {col}'}), 400
            elif col in SKILL_COLUMNS or col == 'Modifier':
                # Update skill or modifier
                df.at[row_index, col] = value
            elif col == 'PPL':
                # Update worker name
                df.at[row_index, col] = value

        # Recalculate shift_duration if times changed
        if 'start_time' in updates or 'end_time' in updates:
            start = df.at[row_index, 'start_time']
            end = df.at[row_index, 'end_time']
            if pd.notnull(start) and pd.notnull(end):
                start_dt = datetime.combine(datetime.today(), start)
                end_dt = datetime.combine(datetime.today(), end)
                if end_dt < start_dt:
                    end_dt += timedelta(days=1)
                df.at[row_index, 'shift_duration'] = (end_dt - start_dt).seconds / 3600

        return jsonify({'success': True})

    except Exception as e:
        return jsonify({'error': str(e)}), 500


@app.route('/api/prep-next-day/add-worker', methods=['POST'])
@admin_required
def add_prep_worker():
    """
    Add a new worker row to working_hours_df.
    """
    try:
        data = request.json
        modality = data.get('modality')
        worker_data = data.get('worker_data', {})

        if modality not in modality_data:
            return jsonify({'error': 'Invalid modality'}), 400

        df = modality_data[modality]['working_hours_df']

        # Build new row
        new_row = {
            'PPL': worker_data.get('PPL', 'Neuer Worker (NW)'),
            'start_time': datetime.strptime(worker_data.get('start_time', '07:00'), '%H:%M').time(),
            'end_time': datetime.strptime(worker_data.get('end_time', '15:00'), '%H:%M').time(),
            'Modifier': float(worker_data.get('Modifier', 1.0)),
        }

        # Add skill columns
        for skill in SKILL_COLUMNS:
            new_row[skill] = int(worker_data.get(skill, 0))

        # Calculate shift_duration
        start_dt = datetime.combine(datetime.today(), new_row['start_time'])
        end_dt = datetime.combine(datetime.today(), new_row['end_time'])
        if end_dt < start_dt:
            end_dt += timedelta(days=1)
        new_row['shift_duration'] = (end_dt - start_dt).seconds / 3600

        # Append to DataFrame
        if df is None or df.empty:
            modality_data[modality]['working_hours_df'] = pd.DataFrame([new_row])
        else:
            modality_data[modality]['working_hours_df'] = pd.concat([df, pd.DataFrame([new_row])], ignore_index=True)

        return jsonify({'success': True, 'row_index': len(modality_data[modality]['working_hours_df']) - 1})

    except Exception as e:
        return jsonify({'error': str(e)}), 500


@app.route('/api/prep-next-day/delete-worker', methods=['POST'])
@admin_required
def delete_prep_worker():
    """
    Delete a worker row from working_hours_df.
    """
    try:
        data = request.json
        modality = data.get('modality')
        row_index = data.get('row_index')

        if modality not in modality_data:
            return jsonify({'error': 'Invalid modality'}), 400

        df = modality_data[modality]['working_hours_df']

        if df is None or row_index >= len(df):
            return jsonify({'error': 'Invalid row index'}), 400

        # Delete row
        modality_data[modality]['working_hours_df'] = df.drop(df.index[row_index]).reset_index(drop=True)

        return jsonify({'success': True})

    except Exception as e:
        return jsonify({'error': str(e)}), 500


@app.route('/api/<modality>/<role>', methods=['GET'])
def assign_worker_api(modality, role):
    modality = modality.lower()
    if modality not in modality_data:
        return jsonify({"error": "Invalid modality"}), 400
    return _assign_worker(modality, role)


@app.route('/api/<modality>/<role>/strict', methods=['GET'])
def assign_worker_strict_api(modality, role):
    modality = modality.lower()
    if modality not in modality_data:
        return jsonify({"error": "Invalid modality"}), 400
    return _assign_worker(modality, role, allow_fallback=False)

def _assign_worker(modality: str, role: str, allow_fallback: bool = True):
    try:
        requested_data = modality_data[modality]
        now = get_local_berlin_now()
        selection_logger.info(
            "Assignment request: modality=%s, role=%s, strict=%s, time=%s",
            modality,
            role,
            not allow_fallback,
            now.strftime('%H:%M:%S'),
        )

        with lock:
            result = get_next_available_worker(
                now,
                role=role,
                modality=modality,
                allow_fallback=allow_fallback,
            )
            if result is not None:
                candidate, used_column, source_modality = result
                actual_modality = source_modality or modality
                d = modality_data[actual_modality]

                candidate = candidate.to_dict() if hasattr(candidate, "to_dict") else dict(candidate)
                if "PPL" not in candidate:
                    raise ValueError("Candidate row is missing the 'PPL' field")
                person = candidate['PPL']

                actual_skill = candidate.get('__skill_source')
                if not actual_skill and isinstance(used_column, str):
                    actual_skill = used_column
                if not actual_skill:
                    actual_skill = role

                selection_logger.info(
                    "Selected worker: %s using column %s (modality %s)",
                    person,
                    actual_skill,
                    actual_modality,
                )

                d['draw_counts'][person] = d['draw_counts'].get(person, 0) + 1
                if actual_skill in SKILL_COLUMNS:
                    if actual_skill not in d['skill_counts']:
                        d['skill_counts'][actual_skill] = {}
                    if person not in d['skill_counts'][actual_skill]:
                        d['skill_counts'][actual_skill][person] = 0
                    d['skill_counts'][actual_skill][person] += 1

                    # Determine if modifier should apply
                    modifier = 1.0
                    modifier_active_only = BALANCER_SETTINGS.get('modifier_applies_to_active_only', False)

                    if modifier_active_only:
                        # Only apply modifier if skill value is 1 (active)
                        skill_value = candidate.get(actual_skill, 0)
                        if skill_value == 1:
                            modifier = candidate.get('Modifier', 1.0)
                    else:
                        # Apply modifier regardless of skill value (old behavior)
                        modifier = candidate.get('Modifier', 1.0)

                    if person not in d['WeightedCounts']:
                        d['WeightedCounts'][person] = 0.0
                    d['WeightedCounts'][person] += (
                        skill_weights.get(actual_skill, 1.0)
                        * modifier
                        * modality_factors.get(actual_modality, 1.0)
                    )

                canonical_id = update_global_assignment(person, actual_skill, actual_modality)
                
                skill_counts = {}
                for skill in SKILL_COLUMNS:
                    if skill in d['skill_counts']:
                        skill_counts[skill] = {w: int(v) for w, v in d['skill_counts'][skill].items()}
                    else:
                        skill_counts[skill] = {}

                worker_pool = set()
                for skill in SKILL_COLUMNS:
                    worker_pool.update(skill_counts.get(skill, {}).keys())

                sum_counts = {}
                for w in worker_pool:
                    total = 0
                    for skill in SKILL_COLUMNS:
                        total += skill_counts[skill].get(w, 0)
                    sum_counts[w] = total

                global_stats = {}
                for worker in sum_counts.keys():
                    global_stats[worker] = get_global_assignments(get_canonical_worker_id(worker))

                result_data = {
                    "Draw Time": now.strftime('%H:%M:%S'),
                    "Assigned Person": person,
                    "Summe": sum_counts,
                    "Global": global_stats,
                    "modality_used": actual_modality,
                    "skill_used": actual_skill,
                    "modality_requested": modality,
                    "fallback_allowed": allow_fallback,
                    "strict_request": not allow_fallback,
                }
                for skill in SKILL_COLUMNS:
                    result_data[skill] = skill_counts.get(skill, {})
            else:
                d = requested_data
                empty_counts = {w: 0 for w in d['draw_counts']}
                skill_counts = {skill: empty_counts.copy() for skill in SKILL_COLUMNS}
                sum_counts = {w: 0 for w in d['draw_counts']}

                message = (
                    "Bitte nochmal klicken"
                    if allow_fallback
                    else "Keine Person in dieser Gruppe verfügbar"
                )

                result_data = {
                    "Draw Time": now.strftime('%H:%M:%S'),
                    "Assigned Person": message,
                    "Summe": sum_counts,
                    "Global": {},
                    "modality_requested": modality,
                    "modality_used": None,
                    "skill_used": None,
                    "fallback_allowed": allow_fallback,
                    "strict_request": not allow_fallback,
                }
                for skill in SKILL_COLUMNS:
                    result_data[skill] = skill_counts.get(skill, {})
        return jsonify(result_data)
    except Exception as e:
        app.logger.exception("Error in _assign_worker")
        return jsonify({"error": str(e)}), 500

@app.route('/edit_info', methods=['POST'])
def edit_info():
    modality = resolve_modality_from_request()
    d = modality_data[modality]
    new_info = request.form.get('info_text', '')
    d['info_texts'] = [line.strip() for line in new_info.splitlines() if line.strip()]
    selection_logger.info(f"Updated info_texts for {modality}: {d['info_texts']}")
    return redirect(url_for('upload_file', modality=modality))

@app.route('/download')
def download_file():
    modality = resolve_modality_from_request()
    d = modality_data[modality]
    return send_from_directory(app.config['UPLOAD_FOLDER'], d['last_uploaded_filename'], as_attachment=True)

@app.route('/download_latest')
def download_latest():
    modality = resolve_modality_from_request()
    backup_dataframe(modality)  # always ensure the latest backup is current
    backup_file = os.path.join(app.config['UPLOAD_FOLDER'], "backups", f"SBZ_{modality.upper()}_live.xlsx")
    if os.path.exists(backup_file):
        return send_from_directory(
            os.path.join(app.config['UPLOAD_FOLDER'], "backups"),
            os.path.basename(backup_file),
            as_attachment=True
        )
    else:
        return jsonify({"error": "Backup file unavailable."}), 404

@app.route('/edit', methods=['POST'])
def edit_entry():
    modality = resolve_modality_from_request()
    d = modality_data[modality]
    idx_str = request.form.get('index')
    person  = request.form['person']
    time_str= request.form['time']
    modifier_str = request.form.get('modifier', '1.0').strip().replace(',', '.')
    new_modifier = float(modifier_str) if modifier_str else 1.0

    with lock:
        if d['working_hours_df'] is None:
            return redirect(url_for('upload_file', modality=modality))

        if idx_str:
            idx = int(idx_str)
            if 0 <= idx < len(d['working_hours_df']):
                old_person = d['working_hours_df'].at[idx, 'PPL']
                old_canonical = get_canonical_worker_id(old_person)
                new_canonical = get_canonical_worker_id(person)
                
                d['working_hours_df'].at[idx, 'PPL'] = person
                d['working_hours_df'].at[idx, 'canonical_id'] = new_canonical
                d['working_hours_df'].at[idx, 'TIME'] = time_str
                d['working_hours_df'].at[idx, 'Modifier'] = new_modifier
                d['worker_modifiers'][person] = new_modifier

                # Ensure all SKILL_COLUMNS exist in the dataframe
                for skill in SKILL_COLUMNS:
                    form_key = SKILL_FORM_KEYS.get(skill, skill.lower())
                    val_str = request.form.get(form_key, '0').strip()
                    new_val = int(val_str) if val_str else 0
                    
                    # Add column if it doesn't exist
                    if skill not in d['working_hours_df'].columns:
                        d['working_hours_df'][skill] = 0
                        
                    d['working_hours_df'].at[idx, skill] = new_val

                if person != old_person:
                    d['draw_counts'][person] = d['draw_counts'].get(person, 0) + d['draw_counts'].pop(old_person, 0)
                    for skill in SKILL_COLUMNS:
                        # Ensure both old and new persons exist in all skill dictionaries
                        if skill not in d['skill_counts']:
                            d['skill_counts'][skill] = {}
                        if old_person not in d['skill_counts'][skill]:
                            d['skill_counts'][skill][old_person] = 0
                        if person not in d['skill_counts'][skill]:
                            d['skill_counts'][skill][person] = 0
                            
                        d['skill_counts'][skill][person] = d['skill_counts'][skill].get(person, 0) + d['skill_counts'][skill].pop(old_person, 0)
                    d['WeightedCounts'][person] = d['WeightedCounts'].get(person, 0) + d['WeightedCounts'].pop(old_person, 0)
                
        else:
            # This is for adding a new row - similar fixes needed here
            canonical_id = get_canonical_worker_id(person)
            data_dict = {
                'PPL': person,
                'canonical_id': canonical_id,
                'TIME': time_str,
                'Modifier': new_modifier,
            }
            for skill in SKILL_COLUMNS:
                form_key = SKILL_FORM_KEYS.get(skill, skill.lower())
                val_str = request.form.get(form_key, '0').strip()
                data_dict[skill] = int(val_str) if val_str else 0
            new_row = pd.DataFrame([data_dict])
            
            # Add missing columns to working_hours_df if needed
            for skill in SKILL_COLUMNS:
                if skill not in d['working_hours_df'].columns:
                    d['working_hours_df'][skill] = 0
                    
            d['working_hours_df'] = pd.concat([d['working_hours_df'], new_row], ignore_index=True)
            if person not in d['draw_counts']:
                d['draw_counts'][person] = 0
            for skill in SKILL_COLUMNS:
                if skill not in d['skill_counts']:
                    d['skill_counts'][skill] = {}
                if person not in d['skill_counts'][skill]:
                    d['skill_counts'][skill][person] = 0
            if person not in d['WeightedCounts']:
                d['WeightedCounts'][person] = 0.0
            d['worker_modifiers'][person] = new_modifier

        d['working_hours_df']['start_time'], d['working_hours_df']['end_time'] = zip(*d['working_hours_df']['TIME'].map(parse_time_range))
        d['working_hours_df']['shift_duration'] = d['working_hours_df'].apply(
            lambda row: (datetime.combine(datetime.min, row['end_time']) - datetime.combine(datetime.min, row['start_time'])).total_seconds() / 3600.0,
            axis=1
        )
        d['total_work_hours'] = d['working_hours_df'].groupby('PPL')['shift_duration'].sum().to_dict()
        
        # Update live backup after editing
        backup_dataframe(modality)

    return redirect(url_for('upload_file', modality=modality))
@app.route('/delete', methods=['POST'])
def delete_entry():
    modality = resolve_modality_from_request()
    d = modality_data[modality]
    idx = int(request.form['index'])
    with lock:
        if d['working_hours_df'] is not None and 0 <= idx < len(d['working_hours_df']):
            d['working_hours_df'].at[idx, 'TIME'] = '00:00-00:00'
            d['working_hours_df'].at[idx, 'start_time'], d['working_hours_df'].at[idx, 'end_time'] = parse_time_range('00:00-00:00')
            d['working_hours_df']['shift_duration'] = d['working_hours_df'].apply(
                lambda row: (datetime.combine(datetime.min, row['end_time']) - datetime.combine(datetime.min, row['start_time'])).total_seconds() / 3600.0,
                axis=1
            )
            d['total_work_hours'] = d['working_hours_df'].groupby('PPL')['shift_duration'].sum().to_dict()
            # Update live backup after deletion
            backup_dataframe(modality)
    return redirect(url_for('upload_file', modality=modality))

@app.route('/get_entry', methods=['GET'])
def get_entry():
    modality = resolve_modality_from_request()
    d = modality_data[modality]

    idx = request.args.get('index', type=int)
    if d['working_hours_df'] is not None and idx is not None and 0 <= idx < len(d['working_hours_df']):
        entry = d['working_hours_df'].iloc[idx]

        # Start building the response with core fields:
        resp = {
            'person':   entry.get('PPL', ''),       # or entry['PPL'] if guaranteed to exist
            'time':     entry.get('TIME', '00:00-00:00'),
            'modifier': entry.get('Modifier', 1.0)
        }

        # Convert skill columns to int safely:
        for skill in SKILL_COLUMNS:
            form_key = SKILL_FORM_KEYS.get(skill, skill.lower())
            if skill in entry:
                val = entry[skill]
                if pd.isna(val):
                    val = 0
                try:
                    resp[form_key] = int(val)
                except (ValueError, TypeError):
                    resp[form_key] = 0
            else:
                resp[form_key] = 0

        return jsonify(resp)

    # If index is out of range or DataFrame is empty:
    return jsonify({"error": "Ungültiger Index"}), 400



@app.route('/api/quick_reload', methods=['GET'])
def quick_reload():
    # Check if this is a skill-based view request
    skill_param = request.args.get('skill')

    if skill_param:
        # Skill-based view: return available modalities for this skill
        skill = normalize_skill(skill_param)
        available_modalities_dict = get_available_modalities_for_skill(skill)
        checks = run_operational_checks('reload', force=True)
        return jsonify({
            "available_modalities": available_modalities_dict,
            "operational_checks": checks,
        })

    # Modality-based view (existing logic)
    modality = resolve_modality_from_request()
    d = modality_data[modality]
    now = get_local_berlin_now()
    checks = run_operational_checks('reload', force=True)

    # Determine available buttons based on currently active working hours
    available_buttons = {SKILL_SLUG_MAP[skill]: False for skill in SKILL_COLUMNS}
    if d['working_hours_df'] is not None:
        tnow = now.time()
        active_df = d['working_hours_df'][
            (d['working_hours_df']['start_time'] <= tnow) &
            (d['working_hours_df']['end_time'] >= tnow)
        ]
        for skill in SKILL_COLUMNS:
            slug = SKILL_SLUG_MAP[skill]
            available_buttons[slug] = bool((skill in active_df.columns) and (active_df[skill].sum() > 0))

    for entry in SKILL_TEMPLATES:
        if entry['always_visible']:
            available_buttons[entry['slug']] = True
            
    # Rebuild per-skill counts:
    skill_counts = {}
    for skill in SKILL_COLUMNS:
        skill_counts[skill] = d['skill_counts'].get(skill, {})

    # Summation per worker
    sum_counts = {}
    worker_pool = set()
    for skill in SKILL_COLUMNS:
        worker_pool.update(skill_counts.get(skill, {}).keys())

    for worker in worker_pool:
        total = 0
        for s in SKILL_COLUMNS:
            total += int(skill_counts[s].get(worker, 0))
        sum_counts[worker] = total

    # Global assignments per worker:
    global_stats = {}
    for worker in sum_counts.keys():
        cid = get_canonical_worker_id(worker)
        global_stats[worker] = get_global_assignments(cid)
        
    # Also compute global weighted counts:
    global_weighted_counts = {}
    for worker in sum_counts.keys():
        canonical = get_canonical_worker_id(worker)
        global_weighted_counts[worker] = get_global_weighted_count(canonical)

    payload = {
        "Draw Time": now.strftime("%H:%M:%S"),
        "Assigned Person": None,
        "Summe": sum_counts,
        "Global": global_stats,
        "GlobalWeighted": global_weighted_counts,
        "available_buttons": available_buttons,
        "operational_checks": checks,
    }
    for skill in SKILL_COLUMNS:
        payload[skill] = skill_counts.get(skill, {})

    return jsonify(payload)


# ============================================================================
# Worker Skill Roster Management API
# ============================================================================

@app.route('/api/admin/skill_roster', methods=['GET'])
@admin_required
def get_skill_roster():
    """Get STAGED worker skill roster (for planning purposes)."""
    try:
        # Load staged roster (not active)
        staged_roster = load_worker_skill_json(use_staged=True)

        # Get available skills and modalities from config
        config = _build_app_config()
        skills = list(config.get('skills', {}).keys())
        modalities = list(config.get('modalities', {}).keys())

        return jsonify({
            'success': True,
            'roster': staged_roster,
            'skills': skills,
            'modalities': modalities,
            'is_staged': True  # Flag to indicate this is staged data
        })
    except Exception as exc:
        selection_logger.error(f"Error getting skill roster: {exc}")
        return jsonify({'success': False, 'error': str(exc)}), 500


@app.route('/api/admin/skill_roster', methods=['POST'])
@admin_required
def save_skill_roster():
    """Save worker skill roster to STAGED JSON (planning only, not immediately active)."""
    try:
        data = request.get_json()
        if not data or 'roster' not in data:
            return jsonify({'success': False, 'error': 'No roster data provided'}), 400

        roster_data = data['roster']

        # Validate roster structure (basic validation)
        if not isinstance(roster_data, dict):
            return jsonify({'success': False, 'error': 'Roster must be a dictionary'}), 400

        # Save to STAGED JSON file (not active)
        if save_worker_skill_json(roster_data, use_staged=True):
            selection_logger.info(f"Worker skill roster STAGED: {len(roster_data)} workers (not yet active)")

            return jsonify({
                'success': True,
                'message': f'Roster changes staged ({len(roster_data)} workers) - Use "Activate" to apply'
            })
        else:
            return jsonify({'success': False, 'error': 'Failed to save staged roster'}), 500

    except Exception as exc:
        selection_logger.error(f"Error saving skill roster: {exc}")
        return jsonify({'success': False, 'error': str(exc)}), 500


@app.route('/api/admin/skill_roster/reload', methods=['POST'])
@admin_required
def reload_skill_roster():
    """Reload STAGED worker skill roster from JSON file."""
    try:
        staged_roster = load_worker_skill_json(use_staged=True)
        return jsonify({
            'success': True,
            'message': f'Staged roster reloaded ({len(staged_roster)} workers)'
        })
    except Exception as exc:
        selection_logger.error(f"Error reloading staged roster: {exc}")
        return jsonify({'success': False, 'error': str(exc)}), 500


@app.route('/api/admin/skill_roster/activate', methods=['POST'])
@admin_required
def activate_skill_roster():
    """Activate staged roster - copy staged → active and reload into memory."""
    global worker_skill_json_roster

    try:
        # Load staged roster
        staged_roster = load_worker_skill_json(use_staged=True)

        # Save to active file
        if not save_worker_skill_json(staged_roster, use_staged=False):
            return jsonify({'success': False, 'error': 'Failed to save to active roster'}), 500

        # Reload active roster into memory
        worker_skill_json_roster = load_worker_skill_json(use_staged=False)

        selection_logger.info(f"Skill roster activated: {len(worker_skill_json_roster)} workers now active")

        return jsonify({
            'success': True,
            'message': f'Roster activated successfully ({len(worker_skill_json_roster)} workers)'
        })
    except Exception as exc:
        selection_logger.error(f"Error activating skill roster: {exc}")
        return jsonify({'success': False, 'error': str(exc)}), 500


@app.route('/skill_roster')
@admin_required
def skill_roster_page():
    """Admin page for managing worker skill roster (planning mode)."""
    return render_template('skill_roster.html')


@app.route('/admin/live-edit')
@admin_required
def live_edit_page():
    """Admin page for live editing of current workers (IMMEDIATE EFFECT)."""
    return render_template('live_edit.html')


@app.route('/api/live_edit/workers', methods=['GET'])
@admin_required
def get_live_edit_workers():
    """Get current workers for a modality (for live editing)."""
    modality = request.args.get('modality', 'ct')
    modality = normalize_modality(modality)

    d = modality_data[modality]

    if d['working_hours_df'] is None or d['working_hours_df'].empty:
        return jsonify({
            'success': True,
            'workers': [],
            'modality': modality
        })

    # Convert DataFrame to list of dicts
    workers_list = []
    for idx, row in d['working_hours_df'].iterrows():
        worker_dict = {
            'index': idx,
            'PPL': row.get('PPL', ''),
            'start_time': row['start_time'].strftime('%H:%M:%S') if pd.notnull(row.get('start_time')) else '',
            'end_time': row['end_time'].strftime('%H:%M:%S') if pd.notnull(row.get('end_time')) else '',
            'shift_duration': row.get('shift_duration', 0),
            'Modifier': row.get('Modifier', 1.0),
            'Normal': int(row.get('Normal', 0)),
            'Notfall': int(row.get('Notfall', 0)),
            'Privat': int(row.get('Privat', 0)),
            'Herz': int(row.get('Herz', 0)),
            'Msk': int(row.get('Msk', 0)),
            'Chest': int(row.get('Chest', 0))
        }
        workers_list.append(worker_dict)

    return jsonify({
        'success': True,
        'workers': workers_list,
        'modality': modality
    })


@app.route('/timetable')
def timetable():
    modality = resolve_modality_from_request()
    d = modality_data[modality]
    if d['working_hours_df'] is not None:
        df_for_json = d['working_hours_df'].copy()
        df_for_json['start_time'] = df_for_json['start_time'].apply(lambda t: t.strftime('%H:%M:%S') if pd.notnull(t) else "")
        df_for_json['end_time'] = df_for_json['end_time'].apply(lambda t: t.strftime('%H:%M:%S') if pd.notnull(t) else "")
        debug_data = df_for_json.to_json(orient='records')
    else:
        debug_data = "[]"
    return render_template('timetable.html', debug_data=debug_data, modality=modality)







app.config['DEBUG'] = True

# Initialize worker skill JSON roster
def init_worker_skill_roster():
    """Load worker skill overrides from JSON on startup."""
    global worker_skill_json_roster
    worker_skill_json_roster = load_worker_skill_json()


# Initialize scheduler for auto-preload
def init_scheduler():
    """Initialize and start background scheduler for auto-preload."""
    global scheduler
    if scheduler is None:
        scheduler = BackgroundScheduler()
        # Run daily at 7:30 AM (Berlin time)
        scheduler.add_job(
            auto_preload_job,
            CronTrigger(hour=7, minute=30, timezone='Europe/Berlin'),
            id='auto_preload',
            name='Auto-preload next workday',
            replace_existing=True
        )
        scheduler.start()
        selection_logger.info("Scheduler started: Auto-preload will run daily at 7:30 AM")

# Initialize worker skill roster from JSON
init_worker_skill_roster()

# Start scheduler when app starts
init_scheduler()

if __name__ == '__main__':
    app.run()

    
    